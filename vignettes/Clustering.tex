% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
  \usepackage{amssymb}
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Techniques for Evaluating Clustering Data in R. The Clustering Package},
  pdfauthor={Luis Alfonso Pérez, Ángel Miguel García Vico, Pedro González and Cristóbal J. Carmona},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{cleveref}
\usepackage{longtable}
\usepackage{colortbl}
\definecolor{green}{rgb}{0.55, 0.71, 0.0}
\newcommand{\gray}{\rowcolor[gray]{.90}}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[3] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\title{Techniques for Evaluating Clustering Data in R. The
\textbf{Clustering} Package}
\author{Luis Alfonso Pérez, Ángel Miguel García Vico, Pedro González and
Cristóbal J. Carmona}
\date{10/11/2020}

\begin{document}
\maketitle
\begin{abstract}
Clustering is a class of frequently studied and applied unsupervised
learning methods whose purpose is a division of data into groups of
similar objects. This technique is quite common among researchers as it
allows to obtain knowledge quickly and easily. The use of this technique
is suitable for automatic data classification in order to reveal
concentrations of data. This paper presents the \textbf{Clustering}
package which contains a set of clustering algorithms with two
objectives: first, grouping data in a homogeneous way by establishing
differences between clusters; and second, generating a ranking between
algorithms and the attributes analyzed in the dataset. This package
contains references to other R packages without using external software.
As a complement to the standard execution through the console, it
incorporates a GUI through which we can execute the package without
having to know the parameters.
\end{abstract}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Exploring the properties of information in order to generate groups is
an unsupervised learning technique known as clustering {[}1{]} {[}2{]}.
This technique is a concise data model where a set of data must be
partitioned and introduced into groups or clusters. These clusters must
meet two conditions: clusters must be as disparate as possible; and the
elements that contain them as similar as possible. Throughout the
literature related to clustering we can see that there are multiple
fields where they can be applied, among which we highlight the
following: Identifying tourists and analyzing their destination patterns
from location-based social media data {[}3{]}; developing clustering
algorithms that maximize performance on 5G heterogeneous networks
{[}4{]}; the application of data mining techniques to agriculture data
{[}5{]}; weighting characteristics based on the strength between
categories and within categories for the analysis of feelings {[}6{]};
music classification, genres and taste patterns {[}7{]}; predicting the
direction of fluctuation, maximum, minimum and closing prices of the
daily exchange rate of bitcoins {[}8{]}; and \texttt{Clustering} people
in a given social network based on textual similarity {[}9{]}.

As a rule clustering algorithms are based on the optimization of an
objective function, which is usually the weighted sum of the distance to
the centers, although these functions may vary and in some cases consist
of the definition of functions. In the literature we can group the data
in different ways among which we highlight {[}10{]}: partitional,
hierarchical or based on density. One of the best known algorithms that
solves the clustering problem is the k-means {[}11{]}.

A wide variety of frameworks have been presented in the literature based
on clustering algorithms such as: Weka {[}12{]}, ClustVis {[}13{]} and
Keel {[}14{]}, among others. Also within R there is a specific Cluster
task view. Inside the Cluster task view we can see two well
differentiated parts: on the one hand we have the most outstanding
packages by functionality and in the other hand we observe the ordered
set of packages that work with clusters. Among the set of packages we
highlight the following: \textbf{ClusterR} {[}15{]} {[}16{]},
\textbf{apcluster} {[}17{]} {[}18{]}, \textbf{cluster} {[}19{]},
\textbf{advclust} {[}20{]} as well as alternatives to the traditional
implementation of k-means and agglomerative hierarchical clustering.
Usually the task of comparing clustering algorithms is tedious, as it
must be performed manually. This is quite time-consuming and in some
cases there can be problems in transmitting the results. Similarly, when
evaluating the distribution of data in clusters, it is necessary to
indicate a categorical variable, so the selection of one variable or
another from a data set can influence the results.

This paper presents the \textbf{Clustering} package. It is a package
that allows us to compare multiple clustering algorithms simultaneously
and assess the accuracy of the results. The purpose of this package is
to allow the evaluation of a set of datasets in order to determine which
attributes are most suitable for clustering. So, we can perform
evaluations of the clusters created, how they have been distributed,
whether the distributions are uniform and how they have been categorized
from the data.

The structure of this contribution is as follows: Firstly, in section
\nameref{sec:clustering} we present the concepts of clustering, types of
clustering and similarity measures. Section \nameref{sec:seccion2}
presents the definition of the evaluation measures in order to value the
distribution of the data in the clusters. Finally, Section
\nameref{sec:seccion3} describes the structure of the package and
presents a complete example about the use of the package.

\hypertarget{clustering}{%
\section{\texorpdfstring{Clustering
\label{sec:clustering}}{Clustering }}\label{clustering}}

Cluster analysis is an unsupervised learning method that constitutes a
cornerstone of an intelligent data analysis process. It is used for the
exploration of inter-relationships among a collection of patterns, by
organizing them into homogeneous clusters. It is called unsupervised
learning because, unlike classification (known as supervised learning),
no a priori labeling of some of the patterns is available to use in
categorizing others and inferring the cluster structure of the whole
data set {[}21{]}. The basic concept of clustering should be expressed
as follows:

``Clustering is the process of identifying natural clusters or clusters
within multidimensional data based on some measure of similarity
(Euclidean, Manhattan, etc.) {[}22{]}.''

This is a base definition of clustering so variations in the problem
definition can be significant, depending mostly on the model specified.
For example, a generative model should define similarity based on a
probabilistic generative mechanism, while a distance-based approach will
use a traditional distance function to quantify it. In addition, the
types of data specified have a significant impact on the problem
definition.

\hypertarget{clustering-types}{%
\subsection{Clustering types}\label{clustering-types}}

There are a variety of clustering algorithms that can be classified
into: hierarchical, partitioning, density-based, grid-based and
probability distribution {[}23{]}.

\begin{itemize}
\item
  Hierarchical clustering: Creates a hierarchical breakdown of data into
  a dendogram that recursively divides the data set into smaller and
  smaller groups. It can be created in two ways: bottom-up or top-down
  {[}24{]}. With the bottom-up method trees are known as agglomerative,
  as the objects are successively combined according to the
  measurements, until they are all joined into one or meet a completion
  condition. In the case of top-down, it is known as divisive, where all
  the objects are in the same group, and as we iterate they are divided
  into smaller subsets until each object is in an individual group or
  fulfills a condition of completion. An example of this type of
  clustering can be found in Figure \textasciitilde{}\ref{hierarchical}.
  Some hierarchical grouping algorithms that belong to this sorting mode
  are: CURE {[}25{]}, CHAMELEON {[}26{]}, and BIRCH {[}27{]}.\\
  \newpage

  \begin{figure}
  \centering
  \includegraphics[width=\textwidth,height=1.92708in]{img/hierarchical.pdf}
  \caption{Hierarchical Clustering. \label{hierarchical}}
  \end{figure}
\item
  Partitional clustering: Is considered to be the most popular of the
  clustering algorithms. Such an algorithm is known as an iterative
  relocation algorithm. This algorithm minimizes a given clustering
  criterion by iteratively relocating data points between clusters until
  an optimal partition is reached. This type of algorithm divides the
  data points into a partition k, where each partition represents a
  cluster. Partial clustering organizes the objects within k clusters so
  that the total deviation of each object from the center of its cluster
  or from a cluster distribution is minimal. The deviation of a given
  point can be evaluated differently according to the algorithm, and is
  generally known as a similarity function. If we want to observe
  graphically how this type of clustering works we can see it in Figure
  \textasciitilde{}\ref{partitional}. Among the partitioning clustering
  algorithms we can find CLARANS, CLARA {[}28{]}, K-prototype {[}29{]},
  K-mode {[}30{]} and K-means {[}31{]}.

  \begin{figure}
  \centering
  \includegraphics[width=\textwidth,height=2.70833in]{img/partitional.pdf}
  \caption{Partitional Clustering. \label{partitional}}
  \end{figure}
\item
  Density-based algorithms: Obtain clusters based on dense regions of
  objects in the data space that are separated by low-density regions
  (these isolated elements represent noise). These regions are
  represented in Figure \textasciitilde{}\ref{density}. Among the
  density-based algorithms, we highlight the following: Dbscan {[}32{]},
  and Denclue {[}33{]}. \newpage

  \begin{figure}
  \centering
  \includegraphics[width=\textwidth,height=2.5in]{img/density.pdf}
  \caption{Density Clustering. \label{density}}
  \end{figure}
\item
  Grid-based clustering: First quantizes the clustering space into a
  finite number of cells and then performs the required operations on
  the quantized space {[}34{]}. Cells that contain more than a certain
  number of points are treated as dense, and the dense cells are
  connected to form the clusters. Some of the best known grid-based
  clustering algorithms include: STING {[}35{]}, Wave Cluster {[}36{]}
  and CLIQUE {[}37{]}.
\item
  Model-based methods: Are primarily based on a probability
  distribution. To be able to measure similarity it is based on the mean
  values that the algorithm tries to minimize with the square error
  function. The Auto Class algorithm uses the Bayesian approach,
  starting with a random initialization of parameters that is gradually
  adjusted in order to find the maximum probability estimates. Among the
  model-based algorithms we highlight SOM {[}38{]}. Model-based
  clustering is shown in Figure \textasciitilde{}\ref{model}.

  \begin{figure}
  \centering
  \includegraphics[width=\textwidth,height=2.70833in]{img/model.pdf}
  \caption{Model-Based Clustering. \label{model}}
  \end{figure}

  \newpage
\end{itemize}

\hypertarget{dissimilarity-measures}{%
\subsection{Dissimilarity measures}\label{dissimilarity-measures}}

Dissimilarity measurements are important in the creation of clusters
with the closest neighbors and the detection of anomalies, and they are
also used in a large number of data mining techniques. It is a measure
that determines the degree to which objects are different. We often use
the term distance as a synonym for dissimilarity. The values of
dissimilarity should be in the range {[}0,1{]}, but it is common to find
in some cases a range other than {[}0,1{]}, therefore it is recommended
to normalize the values in the range {[}0,1{]}.

Many distance measures have been proposed in the literature for data
clustering. Choosing an appropriate similarity measure is crucial for
cluster analysis, especially for a particular type of algorithm. For
example, the density-based clustering algorithms, such as DBScan
{[}32{]}, rely heavily on the similarity computation. Density-based
clustering finds clusters as dense areas in the data set, and the
density of a given point is in turn estimated as the closeness of the
corresponding data object to its neighboring objects {[}39{]}
{[}40{]}.\\
As measures of dissimilarity in clustering we highlight the following:

\begin{itemize}
\tightlist
\item
  Minkowski: Is a metric in a normalized vector space which can be
  considered as a generalization of both the Euclidean distance and the
  Manhattan distance {[}41{]}. \begin{equation}
        d_{min} = (\sum_{i=1}^{n}|x_i - y_i|^m)^\frac{1}{m}, m\geq 1
    \end{equation} where m is a positive real number and \(x_i\) and
  \(y_i\) are two vectors in n-dimensional space.
\item
  Euclidan distance: When in Minkowski metric the value of m is equal to
  2 is calculated Euclidean distance. It is a measure of the true
  straight line distance between two points in a Euclidean space
  {[}24{]}. \begin{equation}
        d_{euc} =  \sqrt{\sum_{i=1}^{n}(x_{i}-y_{i})^2}
    \end{equation}
\item
  Manhattan distance: The m parameter of the Minkowski Distance when is
  1, represent Manhattan Distance. Known as the geometry cab driver is
  defined as the sum of the lengths of the projections of the line
  segment between the points onto the coordinate axes {[}42{]}.
  \begin{equation}
        d_{man} =  \sum_{i=1}^{n}|x_{i}-y_{i}|
    \end{equation}
\item
  Mahalanobis distance: Is a data-driven measure in contrast to the
  Euclidean and Manhattan distances which are independent. It is tasked
  with measuring the distance in a multivariate space {[}42{]}.
  \begin{equation}
        d_{mah}= \sqrt{(x-y)S^-1(x-y)^T}
    \end{equation} where S is the covariance matrix of the dataset.
\item
  Pearson correlation: Pearson correlation: A statistically based metric
  that measures the linear correlation between two variables, x and y
  {[}43{]}. \begin{equation}
        Pearson(x,y)= \frac{\sum_{i=1}^n(x_i-\mu_x)(y_i-\mu_y)}{\sqrt{\sum_{i=1}^n(x_i-y_i)^2}\sqrt{\sum_{i=1}^n(x_i-y_i)^2}}
    \end{equation} where \(\mu_x\) and \(\mu_y\) are the means for x and
  y respectively.
\item
  Jaccard Index: Is a classical similarity measure performed on sets
  with several practical applications in information retrieval, data
  mining, machine learning, and many more {[}44{]} {[}45{]}. t measures
  the similarity of the two data elements as the intersection divided by
  the union of the data elements, as shown below. \begin{equation}
        J(A,B) = \frac{A \cap B}{A \cup B}
    \end{equation}
\item
  Gower distance: It is a measure of similarity that allows the
  simultaneous use of quantitative, qualitative and dichotomous
  variables. By applying this similarity coefficient we can determine
  the degree of similarity between individuals who have had qualitative,
  quantitative characteristics (continuous and discrete) and binary
  characteristics measured. \begin{equation}
        d_{ij}=\sqrt{(1-S_{ij})}
    \end{equation} where \(S_{ij}\) the Gower similarity coefficient is
  {[}46{]}. \begin{equation}
        S_{ij}=\frac{\sum_{k}^{n}w_{ijk}S_{ijk}}{\sum_{n}^{k}w_{ijk}}
    \end{equation} \(S_{ijk}\) denotes the contribution provided by the
  k-th variable, and \(w_{ijk}\) is usually 1 or 0 depending on whether
  the comparison is valid for the k-th variable.
\end{itemize}

\hypertarget{internal-and-external-clustering-validation-measures}{%
\section{\texorpdfstring{Internal and External clustering validation
measures
\label{sec:seccion2}}{Internal and External clustering validation measures }}\label{internal-and-external-clustering-validation-measures}}

Clustering validation is a technique for finding a set of clusters that
best fits natural partitions without any class information. The results
of a clustering algorithm are known as cluster validity. The following
criteria must therefore be taken into account when investigating the
validity of clusters. The first criterion is based on external measures,
which involves evaluating the results of a base algorithm in a
pre-specified structure which is imposed on a data set and reflects our
intuition about the structure of clustering of the data set. The second
criterion is based on internal measures where the results of a
clustering algorithm is evaluated in terms of the quantity involved in
the vectors of the dataset itself (e.g.~the proximity matrix). And there
is a third criterion, known as the relative criterion, whose purpose is
to compare the results of execution of an algorithm with another using
different parameters.

When we talk about criteria based on internal measures we must take into
account the criteria of compactness and separation {[}47{]} {[}48{]} as
can be seen in Figure \ref{cohesion}:

\begin{itemize}
\tightlist
\item
  Compactness; the members of each cluster should be as close to each
  other as possible. A common measure of compactness is variance, which
  should be minimized.
\item
  Separation; the clusters themselves should be widely spaced. There are
  three common approaches to measuring the distance between two
  different clusters:

  \begin{itemize}
  \tightlist
  \item
    Single linkage: Measures the distance between the closest members of
    the clusters.
  \item
    Complete linkage: Measures the distance between the most distant
    members.
  \item
    Comparison of centroids: Measures the distance between the centers
    of the clusters.
  \end{itemize}
\end{itemize}

These criteria are graphically represented in the Figure
\ref{separation}.

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=2.32292in]{img/separation.png}
\caption{Intercluster distance {[}49{]}. \label{separation}}
\end{figure}

Within external measures there are some measures to evaluate clustering
results. Among these we highlight:

\begin{itemize}
\item
  Entropy: Entropy: Evaluates the distribution of categories in a
  cluster {[}50{]}. \begin{equation}
        Entropy(j) =  \sum_{j=1}^{m} \frac{|C_{j}|}{n}E_{j}
    \end{equation} Where \(n_{j}\) the cluster size is j, n is the
  number of clusters, and m is the total number of data points. To
  calculate the \texttt{entropy} of a data set we need to calculate the
  class distribution of the objects in each group as follows.
  \begin{equation}
        E_{j} =  \sum_{i} p_{ij}log(p_{ij})
    \end{equation} Where \(p_{ij}\) is the probability of a point in the
  cluster \(i\) of being classified as class \(j\).
\item
  Recall: Indicates the proximity of the measurement results to the true
  value {[}51{]}. \begin{equation}
         Recall(i,j) = \frac{n_{ij}}{|C_{i}|}
    \end{equation} \(n_{ij}\) is the number of objects of class i that
  are in cluster j and \(n_{i}\) is the number of objects in cluster i.
\item
  Precision: Refers to the dispersion of the set of values obtained from
  repeated measurements of one magnitude {[}51{]}. \begin{equation}
            Precision(i,j) = \frac{n_{ij}}{|C_{j}|}
        \end{equation} \(n_{j}\) is the number of objects in cluster j.
\item
  F-measure: Merges the concepts of accuracy and recall of the
  information retrieved. Therefore, we calculate the cluster accuracy
  and recall for each class as. \begin{equation}
        F-measure(i,j) = \frac{2 * (Precision(i,j) * Recall(i,j))}{(Precision(i,j) + Recall(i,j))}
    \end{equation}
\item
  Fowlkes-Mallows Index: It is a measure of comparison of hierarchical
  clustering. However, it can be used in flat clustering since it
  consists of the calculation of an index \(B_{i}\) for each level i = 2
  ,\ldots, n-1 of the hierarchy {[}52{]}. The measure \(B_{i}\) is
  easily generalizable. It can therefore be said that Fowlkes is a
  measure that can be interpreted as the geometric mean of accuracy
  (ratio between the number of relevant objects recovered and the total
  number of objects recovered). \begin{equation}
        Fowlkes(i,j) = \sqrt{Precision(i,j) * Recall(i,j)}
    \end{equation}
\item
  Variation information: Variation in information or distance of shared
  information is a measure of the distance between two groups {[}52{]}.
  This measure is closely related to mutual information (mutual
  dependence between the two variables) {[}53{]}. \begin{equation}
       VI(|C_{i}|,|C_{j}|) = 2H(C_{i},C_{j}) - H(C_{i}) - H(C_{j})
    \end{equation} \(H(C_{i},C_{j})\) is the joint \texttt{entropy} of
  two clusters, \(H(C_{i})\) is the \texttt{entropy} of \(C_{i}\) and
  \(H(C_{j})\) is the \texttt{entropy} of \(C_{j}\).\\
  As with the external measures, we will now list the most relevant
  internal measures:
\item
  Connectivity: This measure reflects the extent to which items placed
  in the same group are considered their closest neighbors in the data
  space, i.e.~the degree of connection of the clusters should be minimal
  {[}54{]}. \begin{equation}
        Connectivity = min_{ 1\leq i \leq K} \left( min_{1\leq j \leq K, i\not= j} \left( \frac{dist(C_i,C_j)}{max_{1\leq k \leq K} \lbrace diam(C_k) \rbrace }  \right) \right)
    \end{equation} Where \(dist(C_i,C_j)\) is the distance between two
  clusters and \(diam(C_k)\) is the diameter of a particular cluster
  {[}54{]}.
\item
  Dunn: It represents the relationship of the smallest distance between
  observations that are not in the same cluster and the largest distance
  within the same cluster {[}55{]}. \begin{equation}
    Dunn =  min_{1_\leq i\leq k} \left( min_{i+1\leq j \leq k}  \left( \frac{dist(C_i,C_j)}{max_{1_\leq l \leq k} diam(C_{k})} \right) \right)
    \end{equation} Where \(dist(C_{i},C_{j})\) is distance between
  clusters \(C_{i}\) and \(C_{j}\) and \(diam(C_{k})\) is the cluster
  diameter \(C_{k}\).\\
\item
  Silhouette index: The silhouette value is a measure of how similar an
  object is to its own cluster (cohesion) compared to other clusters
  (separation) {[}56{]}. \begin{equation}
        S = \frac{1}{N}\sum_{i=0}^{N}\frac{b_{i} - a_{i}}{max(a_{i},b_{i})}
    \end{equation} where
  \[a_{i}=\frac{1}{|C_{j}| - 1} \sum_{y\in C_{j},y\neq x_{i}}^{}\|y-x_{i}\|\]
  and
  \[ b_{i} = \min\limits_{l \in H, l\neq j}^{} \frac{1}{|C_{l}|} \sum_{y \in C_{l}}^{} \| y - x_{i} \| \]
  with \[ x_{i} \in C_{j}, H = \{h: 1 \leq h \leq K\}\]
\end{itemize}

If we look at Figure \textasciitilde{}\ref{external} we can group
\texttt{Entropy, Recall, Precision, F-Measure, Fowlkes-Mallows Index and Variation information}
into three families {[}57{]}:

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=2.70833in]{img/external.pdf}
\caption{External validation methods. \label{external}}
\end{figure}

\begin{itemize}
\item
  Matching Sets: Used to compare two partitions of data. Consists of
  those methods that identify the relationship between each cluster
  detected in C and its natural correspondence to the classes in the
  reference result defined by P (clustering result prediction).\\
  Several measures can be defined for measuring the similarity between
  the clusters in C, obtained by the clustering algorithm, and the
  clusters in P, corresponding to our prior (external) knowledge. The
  metrics included in this method are: \texttt{Precision, Recall} and
  \texttt{F-measure}.
\item
  Peer-to-peer Correlation: Based on the correlation between pairs,
  i.e., they seek to measure the similarity between two partitions under
  equal conditions, such as the result of a grouping process for the
  same set, but by means of two different methods \$ C\_\{i\} \$ and \$
  C\_\{j\} \$. It is assumed that the examples that are in the same
  cluster in \$ C\_\{i\} \$ should be in the same class in \$ C\_\{j\}
  \$, and vice-versa. In our package we use the metric:
  \texttt{Fowlkes-Mallows Index}.
\item
  Measures Based on Information Theory: A third family is based on
  Information Theory concepts, such as the existing uncertainty in the
  prediction of the natural classes provided by partition C. This family
  includes basic measures such as \texttt{Entropy} and
  \texttt{Variation Information}.
\end{itemize}

Internal evaluation metrics (see Figure \textasciitilde{}\ref{cohesion})
do not require external information, so they are focused on measuring
cohesion (how close the elements are to each other) and separation (they
quantify the level of separation between clusters).

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=0.83333in]{img/cohesion.png}
\caption{Representation of cohesion and separation in clustering
{[}57{]}. \label{cohesion}}
\end{figure}

According to the Figure \textasciitilde{}\ref{internal}, the internal
\texttt{Dunn, Silhouette} and \texttt{Connectivity} metrics are based on
the concepts mentioned above so we can group them as partitioning
methods.

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=0.98958in]{img/internal.pdf}
\caption{Internal validation methods {[}57{]}. \label{internal}}
\end{figure}

\hypertarget{the-clustering-package}{%
\section{\texorpdfstring{The \textbf{Clustering} package
\label{sec:seccion3}}{The Clustering package }}\label{the-clustering-package}}

The \textbf{Clustering} package has been written entirely in R language.
The package include other packages with hierarchical, partitional and
agglomerative algorithms. The package has been provided with the ability
to read data in different formats such as CSV,
\href{https://sci2s.ugr.es/keel/references.php}{KEEL},
\href{https://www.cs.waikato.ac.nz/ml/weka/}{ARFF (Weka)} and
\texttt{data.frame} objects. This package implements functionality not
developed until now. In current implementations we cannot run several
algorithms simultaneously. This option is very useful to compare the
results of several algorithms. Another very useful option, not found
until now is the ability to run all the dissimilarity measures
implemented for an algorithm in the same run, e.g., running the K-means
algorithm for the Euclidean and Manhattan dissimilarity measures
simultaneously. This perspective is quite interesting when it comes to
know what measure to use in the execution of the algorithm. In order to
evaluate the quality of the clusters we find packages that perform
external or internal evaluations, but there was no implementation that
give us a joint evaluation of both. Finishing the review we find that
when evaluating the quality of clusters it is necessary to indicate an
attribute of the dataset used in the execution. Depending on the chosen
attribute, results may vary, so it would be important to have methods
that give us the result of running each of the quality measures by
attribute, the measure of dissimilarity in the case of it existing, the
number of clusters and algorithm. With this \textbf{Clustering} package
we will be able to execute simultaneously several algorithms for each of
the implemented dissimilarity measures. In addition, when evaluating the
results of the executions we have a set of measures that are executed
together, which until now was done one execution per measure. It is
possible to incorporate new measures quickly in the future. In addition,
when running the algorithm all the attributes of the dataset will be
executed for each of the quality metrics. The results of the executions
are visually displayed so that you can draw conclusions quickly.

\hypertarget{package-algorithms}{%
\subsection{Package algorithms}\label{package-algorithms}}

These are the algorithms
\footnote{For more information about the advclust, amap, apcluster, cluster, Cluster, gmp, pvclust packages you can visit the following link \url{https://cran.r-project.org/web/packages/available_packages_by_name.html}}
available within the package, which we will classify as follows:

\begin{itemize}
\tightlist
\item
  Hierarchical Clustering:
  \texttt{agnes, clara, daisy, diana, fanny, fuzzy\_cm, fuzzy\_gk, fuzzy\_gg, hcluster, mona, pam, pvpick and pvclust.}
\item
  Partitioning Clustering: \texttt{gama, gmm, kmeans}.
\item
  Aglomerative Clustering: \texttt{aggExCluster, apclusterK}.
\end{itemize}

These are the algorithms included in the packages that have been
mentioned most often in the literature.

\hypertarget{package-architecture}{%
\subsection{Package Architecture}\label{package-architecture}}

The main class of the package is the \texttt{Clustering} object.

\begin{itemize}
\tightlist
\item
  \texttt{clustering()}: This object stores the results of the
  \textbf{Clustering} package execution and contains the following
  properties:

  \begin{itemize}
  \tightlist
  \item
    \texttt{result.} It represents the \texttt{data.frame} with the
    results. In each column we have represented the evaluation metrics
    used to evaluate the clusters. We can see the execution time of
    these metrics, datasets, the correspondence between the evaluation
    metric calculation and the dataset attribute, the measures of
    similarity and the algorithms.
  \item
    \texttt{has\_internal\_metrics.} It is a boolean operator that
    indicates whether internal evaluation measures have been used or
    not.
  \item
    \texttt{has\_external\_metrics.} It is a boolean operator that
    indicates whether external evaluation measures have been used or
    not.
  \item
    \texttt{algorithms\_executed.} It represents a character vector with
    the algorithms executed independently of the package.
  \item
    \texttt{measures\_executed.} It represents a vector of characters
    with the measures of similarity used by the algorithms indicated.
  \end{itemize}
\end{itemize}

This class exports the well-known S3 methods \texttt{print()} and
\texttt{summary()} that show the data structure without codification,
and a summary with basic information about the dataset respectively. We
can perform sorting and filtering operations for further processing of
the results. In any case if we need to perform filtering operations we
can overload the operator (`{[}') in order to perform such operations in
an easier way.

\begin{itemize}
\tightlist
\item
  External metrics: For external metrics we have a set of methods by
  which we can determine the behavior of the algorithms based on the
  best attribute of the data set, measures of dissimilarity and the
  number of clusters. The methods are as follows:

  \begin{itemize}
  \tightlist
  \item
    \texttt{best\_ranked\_external\_metrics()}: With the execution of
    this method we obtain which attribute of the data set has better
    behavior by algorithm, measure of dissimilarity and number of
    clusters.
  \item
    \texttt{evaluate\_best\_validation\_external\_by\_metrics()}: This
    method should be used to group the data by algorithm and
    dissimilarity measure, instead of obtaining the best attribute from
    the data set.
  \item
    \texttt{evaluate\_validation\_external\_by\_metrics()}: Method for
    grouping the results of the execution by algorithms.
  \item
    \texttt{result\_external\_algorithm\_by\_metric()}: It is used for
    obtaining the results of an algorithm indicated as a parameter
    grouped by number of clusters.
  \end{itemize}
\item
  Internal metrics: For internal metrics we have the same set of methods
  as mentioned above for external metrics.

  \begin{itemize}
  \tightlist
  \item
    \texttt{best\_ranked\_internal\_metrics()}: With the execution of
    this method we obtain which attribute of the data set has better
    behavior by algorithm, measure of dissimilarity and number of
    clusters.
  \item
    \texttt{evaluate\_best\_validation\_internal\_by\_metrics()}: This
    method should be used to group the data by algorithm and
    dissimilarity measure, instead of obtaining the best attribute from
    the data set.
  \item
    \texttt{evaluate\_validation\_internal\_by\_metrics()}: Method for
    grouping the results of the execution by algorithms.
  \item
    \texttt{result\_internal\_algorithm\_by\_metric()}: It is used for
    obtaining the results of an algorithm indicated as a parameter
    grouped by number of clusters.
  \end{itemize}
\item
  \texttt{plot\_clustering()}: Method that represents the results of
  clustering in a bar chart. The graph represents the distribution of
  the algorithms based on the number of partitions and the evaluation
  metrics which can be internal or external.
\item
  \texttt{export\_external\_file()}: The results of external metrics can
  be exported in \LaTeX~format, for integration into documents with that
  format.
\item
  \texttt{export\_internal\_file()}: As indicated above, we use this
  method to export the results of the internal metrics.
\end{itemize}

\hypertarget{use-of-clustering-package}{%
\subsection{\texorpdfstring{Use of \textbf{Clustering}
package}{Use of Clustering package}}\label{use-of-clustering-package}}

The fastest way to download the \textbf{Clustering} package and use it
is to use the install instruction.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"Clustering"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A development version is available on the github repository
\url{https://github.com/laperez/Clustering}. To use the development
version you must install the \texttt{devtools} package and use the
\texttt{install\_github method()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{\textquotesingle{}laperez/Clustering\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The main dependencies of the \textbf{Clustering} package are:
\texttt{advclust, amap, apcluster, cluster, ClusterR, gmp and pvclust}.
These are the packages used for implementing the clustering algorithms.
We can find dependencies for data processing and GUI, such as
\texttt{shiny} and \texttt{DT} among others. Once the package is
installed it is necessary to load it in the following way:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"Clustering"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once the installation and loading process has been completed, we proceed
with the processing of the data and its execution.

\hypertarget{load-and-use-of-datasets}{%
\subsection{Load and use of datasets}\label{load-and-use-of-datasets}}

For the execution of the main method of the package we must provide data
which can be in different formats. The file formats accepted by the
package are: KEEL, ARFF and CSV. The data can be loaded in two ways:
firstly we can indicate a directory with files in the formats indicated
above and load all the available files; and secondly we can provide a
\texttt{data.frame} with the necessary data for execution. The code to
read the files in ARFF format it has been extracted from the
\textbf{mldr} package {[}58{]}.

If we need to work with test data, we have pre-loaded data. The loaded
datasets have been obtained from the KEEL repository url
\url{https://sci2s.ugr.es/keel/category.php?cat=uns} in CSV format.

Note that the extension is used to determine the type of file format.

\hypertarget{analysis-of-clustering-methods-using-the-clustering-package}{%
\subsection{\texorpdfstring{Analysis of clustering methods using the
\textbf{Clustering}
package}{Analysis of clustering methods using the Clustering package}}\label{analysis-of-clustering-methods-using-the-clustering-package}}

Once the way to provide the data has been defined the next step is to be
able to execute the main method of the application, which is
\texttt{clustering()}. With this method we can compare the clustering
algorithms included in the aforementioned packages. When comparing we
can do it by packages or simply by indicating the algorithms contained
in them. To evaluate how the data have been distributed in the clusters,
a set of evaluation measures are performed that return numeric values.
One improvement built into the package is that in addition to returning
the numerical value of the metrics, it can return the dataset attribute
corresponding to that value. In addition, the algorithms are executed
for all measures of similarity implemented. All this functionality is
incorporated into the main method. Therefore the parameters of the
\texttt{clustering()} method are the following:

\begin{itemize}
\tightlist
\item
  \texttt{path}: The file path. It is only allowed to use \texttt{path}
  or \texttt{df} but not both at the same time. Only files in
  \texttt{.dat, .csv} or \texttt{.arff} format are allowed.
\item
  \texttt{df}: Data matrix or data frame, or similarity matrix.
\item
  \texttt{packages}: Character vector with the packets running the
  algorithm. The seven packages implemented are: cluster, ClusterR,
  advclust, amap, apcluster, gama, pvclust. By default the system runs
  all packages.
\item
  \texttt{algorithm}: Is an array with the list of the algorithms
  implemented by the packages. The algorithms are:
  \texttt{fuzzy\_cm, fuzzy\_gg,fuzzy\_gk, hclust, apclusterK, agnes, clara, daisy, diana, fanny, mona, pam, gmm, kmeans\_arma, kmeans\_rcpp, mini\_kmeans, gama, pvclust}.
\item
  \texttt{min}: An integer with the minimum number of clusters This data
  is necessary to indicate the minimum number of clusters when grouping
  the data. The default value is 3.
\item
  \texttt{max}: An integer with the maximum number of clusters. This
  data is necessary to indicate the maximum number of clusters when
  grouping the data. The default value is 4.
\item
  \texttt{metrics}: Character vector with the metrics implemented in
  order to evaluate the distribution of the data in clusters. The night
  metrics implemented are:
  \texttt{entropy, variation\_information, precision,recall,f\_measure,fowlkes\_mallows\_index,connectivity,dunn,silhouette}.
\item
  \texttt{metricsAttr}: Character vector with the metrics implemented in
  order to evaluate the distribution of the data in clusters. The nine
  metrics implemented are:
  \texttt{entropy, variation\_information, precision,recall,f\_measure,fowlkes\_mallows\_index,connectivity,dunn,silhouette}.
  This column is the same as the previous one, with the difference that
  it shows the dataset attribute.
\item
  \texttt{attributes}: A boolean datum which indicates whether we want
  to show as a result the attributes of the datasets. The default value
  is F.
\end{itemize}

Once the definition of the input attributes of the \texttt{clustering}
function has been completed, we will carry out a test. To do this we
will use the \texttt{data.frame} Basketball (this dataset contains five
attributes representing the statistics of a set of 96 basketball
players). The attributes are:
\texttt{assists per minute, height, time played, age and points per minute}.
In the output data of the \texttt{clustering} function, these attributes
are numbered from left to right in ascending order starting with one.
The idea of doing it this way is to give it a homogeneity instead of
using the names of the attributes from the dataset. As algorithms used
in the execution we will use \texttt{gmm} and \texttt{fanny} (included
in the \textbf{ClusterR} and \textbf{cluster} packages). We will
indicate a range of partitions between 3 and 5 and evaluate
\texttt{entropy} as an external evaluation measure and \texttt{dunn} as
an internal one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OtherTok{\textless{}{-}}\NormalTok{ Clustering}\SpecialCharTok{::}\FunctionTok{clustering}\NormalTok{(}\AttributeTok{df =}\NormalTok{ basketball, }\AttributeTok{min =} \DecValTok{3}\NormalTok{, }\AttributeTok{max =} \DecValTok{5}\NormalTok{, }\AttributeTok{algorithm =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}gmm\textquotesingle{}}\NormalTok{,}
                        \StringTok{\textquotesingle{}fanny\textquotesingle{}}\NormalTok{), }\AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}entropy\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}dunn\textquotesingle{}}\NormalTok{), }\AttributeTok{attributes =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

The attributes resulting from the execution of the method are:

\begin{itemize}
\tightlist
\item
  \texttt{Algorithm:} Indicates the clustering algorithm used in the
  data processing.

  \item

  \texttt{Distance:} Is the measure of similarity used by the algorithm
  to calculate the similarity between the data.
\item
  \texttt{Clusters:} Is the number of clusters used by the algorithm.
  Used in Partitional Clustering.
\item
  \texttt{Dataset:} Is the name of the \texttt{data.frame}
  \texttt{dataframe} appears by default, but if instead of using the
  \texttt{df} parameter in the clustering method we use path (directory
  with files with extension dat), in the column must appear the names of
  the processed datasets.
\item
  \texttt{timeExternal:} Time taken to implement external evaluation
  measures.
\item
  \texttt{metrics:} Each metric indicated in the execution is presented
  in individual columns. In this case we have both external
  (\texttt{entropy}) and internal (\texttt{dunn}) metrics. Note: in the
  metric field we indicate all the measurements we wish to evaluate. The
  metrics implemented are: \texttt{entropy}, \texttt{recall},
  \texttt{precision}, \texttt{f\_measure},
  \texttt{fowlkes\_mallows\_index}, \texttt{connectivity}, \texttt{dunn}
  and \texttt{silhouette}.
\item
  \texttt{timeInternal:} Time taken to implement internal evaluation
  measures.
\item
  \texttt{timeExternalAttr:} Time taken to implement external evaluation
  measures by attribute.
\item
  \texttt{metricsAttr:} The same metrics that instead of showing
  numerical values show the attributes of the dataset.
\item
  \texttt{timeInternalAttr:} Time taken to implement internal evaluation
  measures by attribute.
\end{itemize}

In Table \textasciitilde{}\ref{tab:clustering} we have the results of
the execution of the \texttt{clustering()} method. According to the
results, the algorithm that better behaves for the \texttt{entropy}
metric is \texttt{gmm}, as highlighted in Table
\textasciitilde{}\ref{tab:clustering}.\\
\clearpage

\begin{longtable}{| p{1.1cm} | p{2cm} | p{0.8cm} | p{1.3cm} | p{0.60cm} | p{0.7cm} | p{0.65cm} | p{0.65cm} | p{0.65cm} | p{0.65cm} | p{0.7cm} | p{0.65cm} |}
\hline
\scriptsize  Algorithm & \scriptsize  Distance  &  \scriptsize Clusters & \scriptsize  Dataset & \scriptsize tE  & \scriptsize entropy & \scriptsize  dunn  & \scriptsize tI & \scriptsize tEAttr  & \scriptsize enAttr & \scriptsize duAttr & \scriptsize tIAttr \\
\hline
\scriptsize     gmm   & \scriptsize   gmm\_euclidean & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0042  &  \scriptsize 0.2374  & \scriptsize 0.1096 & \scriptsize    0.0004  &  \scriptsize    5  & \scriptsize    2  & \scriptsize    1  & \scriptsize    1 \\
\scriptsize     gmm   & \scriptsize   gmm\_euclidean  & \scriptsize    3   &  \scriptsize basketball  & \scriptsize    0.0046  &  \scriptsize 0.2120  & \scriptsize 0.1096 & \scriptsize    0.0004 &   \scriptsize    1  & \scriptsize    4 &  \scriptsize    2 &  \scriptsize    2 \\
\scriptsize     gmm   & \scriptsize   gmm\_euclidean & \scriptsize    3   &  \scriptsize basketball & \scriptsize    0.0079  &  \scriptsize 0.0064  &\scriptsize 0.1096 & \scriptsize    0.0005 &   \scriptsize    3  &  \scriptsize    3  &  \scriptsize    3  & \scriptsize    3 \\
\scriptsize     gmm   & \scriptsize   gmm\_euclidean & \scriptsize    3 &  \scriptsize basketball & \scriptsize    0.0081  &  \scriptsize 0.0032  & \scriptsize 0.1096 & \scriptsize    0.0005 &   \scriptsize    2  & \scriptsize    5  & \scriptsize    4  & \scriptsize    4 \\
\scriptsize     gmm   & \scriptsize   gmm\_euclidean & \scriptsize    3     & \scriptsize basketball & \scriptsize    0.0089  &  \scriptsize 0.0000 &  \scriptsize 0.1096 & \scriptsize    0.0006 &   \scriptsize    4  & \scriptsize    1  & \scriptsize    5  & \scriptsize    5 \\
\cline{3-12}
\scriptsize     gmm   & \scriptsize   gmm\_euclidean & \scriptsize    4     & \scriptsize basketball  & \scriptsize    0.0041  & \scriptsize 0.3734 &  \scriptsize 0.1233 & \scriptsize    0.0004 &   \scriptsize    5 & \scriptsize    2  & \scriptsize    1  & \scriptsize    4 \\
\scriptsize     gmm   & \scriptsize   gmm\_euclidean & \scriptsize    4     & \scriptsize basketball & \scriptsize    0.0041 &   \scriptsize 0.2983 & \scriptsize 0.1233 & \scriptsize    0.0004 &   \scriptsize    2 & \scriptsize    4 &  \scriptsize    2 &  \scriptsize    5 \\
\scriptsize     gmm   & \scriptsize   gmm\_euclidean & \scriptsize    4     & \scriptsize basketball & \scriptsize    0.0071  &  \scriptsize 0.0064 &  \scriptsize 0.1233 & \scriptsize    0.0004 &   \scriptsize    4 &  \scriptsize    3 &  \scriptsize    3 &  \scriptsize    1 \\
\scriptsize     gmm   & \scriptsize   gmm\_euclidean & \scriptsize    4     & \scriptsize basketball & \scriptsize    0.0071   & \scriptsize 0.0032 &  \scriptsize 0.1233 & \scriptsize    0.0004 &   \scriptsize    1 &  \scriptsize    5  & \scriptsize    4  & \scriptsize    2 \\
\scriptsize     gmm   & \scriptsize   gmm\_euclidean & \scriptsize    4     & \scriptsize basketball & \scriptsize    0.0193   & \scriptsize 0.0000 &  \scriptsize 0.1233 & \scriptsize    0.0005 &   \scriptsize    3 &  \scriptsize    1 &  \scriptsize    5 &  \scriptsize    3 \\
\cline{3-12}
\scriptsize     gmm   & \scriptsize   gmm\_euclidean & \scriptsize    5     & \scriptsize basketball & \scriptsize    0.0044   & \scriptsize 0.4175 &  \scriptsize 0.1619 & \scriptsize    0.0004 &   \scriptsize    3 &  \scriptsize    2  & \scriptsize    1  & \scriptsize    1 \\
\scriptsize     gmm   & \scriptsize    gmm\_euclidean & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0045  &  \scriptsize 0.3857 & \scriptsize 0.1619 & \scriptsize    0.0004 & \scriptsize   1  & \scriptsize    4  & \scriptsize    2  & \scriptsize    4 \\
\scriptsize     gmm   & \scriptsize    gmm\_euclidean & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0079  &  \scriptsize 0.0064 & \scriptsize 0.1619 & \scriptsize    0.0004 & \scriptsize   4  & \scriptsize    3  & \scriptsize    3  & \scriptsize    5 \\
\scriptsize     gmm   & \scriptsize    gmm\_euclidean & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0079  &  \scriptsize 0.0032 & \scriptsize 0.1619 & \scriptsize    0.0005 & \scriptsize   2  & \scriptsize    5  & \scriptsize    4  & \scriptsize    3 \\
\scriptsize     gmm   & \scriptsize    gmm\_euclidean & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0091  &  \scriptsize 0.0000 & \scriptsize 0.1619 & \scriptsize    0.0005 & \scriptsize   5  & \scriptsize    1  & \scriptsize    5  & \scriptsize    2 \\
\cline{2-12}
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0030  &  \scriptsize 0.2498 & \scriptsize 0.1151 & \scriptsize    0.0004 & \scriptsize   3  & \scriptsize    2  & \scriptsize    1  & \scriptsize    4 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0036  &  \scriptsize 0.2201 & \scriptsize 0.1151 & \scriptsize    0.0004 & \scriptsize   2  & \scriptsize    4  & \scriptsize    2  & \scriptsize    1 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0067  &  \scriptsize 0.0064 & \scriptsize 0.1151 & \scriptsize    0.0004 & \scriptsize   5  & \scriptsize    3  & \scriptsize    3  & \scriptsize    5 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0073  &  \scriptsize 0.0032 & \scriptsize 0.1151 & \scriptsize    0.0005 & \scriptsize   1  & \scriptsize    5  & \scriptsize    4  & \scriptsize    2 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0075  &  \scriptsize 0.0000 & \scriptsize 0.1151 & \scriptsize    0.0005 & \scriptsize   4  & \scriptsize    1  & \scriptsize    5  & \scriptsize    3 \\
\cline{3-12}
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    4    & \scriptsize basketball & \scriptsize    0.0034  &  \scriptsize 0.3563 & \scriptsize 0.1179 & \scriptsize    0.0004 & \scriptsize   4  & \scriptsize    2  & \scriptsize    1  & \scriptsize    4 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    4    & \scriptsize basketball & \scriptsize    0.0049  &  \scriptsize 0.2919 & \scriptsize 0.1179 & \scriptsize    0.0004 & \scriptsize   2  & \scriptsize    4  & \scriptsize    2  & \scriptsize    1 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    4    & \scriptsize basketball & \scriptsize    0.0068  &  \scriptsize 0.0064 & \scriptsize 0.1179 & \scriptsize    0.0004 & \scriptsize   5  & \scriptsize    3  & \scriptsize    3  & \scriptsize    2 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    4    & \scriptsize basketball & \scriptsize    0.0073  &  \scriptsize 0.0032 & \scriptsize 0.1179 & \scriptsize    0.0005 & \scriptsize   1  & \scriptsize    5  & \scriptsize    4  & \scriptsize    5 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    4    & \scriptsize basketball & \scriptsize    0.0076  &  \scriptsize 0.0000 & \scriptsize 0.1179 & \scriptsize    0.0007 & \scriptsize   3  & \scriptsize    1  & \scriptsize    5  & \scriptsize    3 \\
\cline{3-12}
\rowcolor{green} \scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0032  &  \scriptsize 0.4290 & \scriptsize 0.1141 & \scriptsize    0.0004 & \scriptsize   4  & \scriptsize    2  & \scriptsize    1  & \scriptsize    1 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0036  &  \scriptsize 0.3887 & \scriptsize 0.1141 & \scriptsize    0.0004 & \scriptsize   1  & \scriptsize    4  & \scriptsize    2  & \scriptsize    5 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0072  &  \scriptsize 0.0064 & \scriptsize 0.1141 & \scriptsize    0.0005 & \scriptsize   3  & \scriptsize    3  & \scriptsize    3  & \scriptsize    4 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0079  &  \scriptsize 0.0032 & \scriptsize 0.1141 & \scriptsize    0.0007 & \scriptsize   2  & \scriptsize    5  & \scriptsize    4  & \scriptsize    2 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0084  &  \scriptsize 0.0000 & \scriptsize 0.1141 & \scriptsize    0.0011 & \scriptsize   5  & \scriptsize    1  & \scriptsize    5  & \scriptsize    3 \\
\cline{1-12}
\scriptsize   fanny   & \scriptsize  fanny\_euclidean & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0107  &  \scriptsize 0.2069 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   5  & \scriptsize    4  & \scriptsize    1  & \scriptsize    1 \\
\scriptsize   fanny   & \scriptsize  fanny\_euclidean & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0114  &  \scriptsize 0.1675 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   2  & \scriptsize    2  & \scriptsize    2  & \scriptsize    2 \\
\scriptsize   fanny   & \scriptsize  fanny\_euclidean & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0140  &  \scriptsize 0.0032 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   4  & \scriptsize    3  & \scriptsize    3  & \scriptsize    3 \\
\scriptsize   fanny   & \scriptsize  fanny\_euclidean & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0159  &  \scriptsize 0.0032 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   1  & \scriptsize    5  & \scriptsize    4  & \scriptsize    4 \\
\scriptsize   fanny   & \scriptsize  fanny\_euclidean & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0161  &  \scriptsize 0.0000 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   3  & \scriptsize    1  & \scriptsize    5  & \scriptsize    5 \\
\cline{3-12}
\scriptsize   fanny   & \scriptsize  fanny\_euclidean & \scriptsize    4    & \scriptsize basketball & \scriptsize    0.0123  &  \scriptsize 0.2069 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   3  & \scriptsize    4  & \scriptsize    1  & \scriptsize    1 \\
\scriptsize   fanny   & \scriptsize  fanny\_euclidean & \scriptsize    4    & \scriptsize basketball & \scriptsize    0.0128  &  \scriptsize 0.1675 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   1  & \scriptsize    2  & \scriptsize    2  & \scriptsize    2 \\
\scriptsize   fanny   & \scriptsize  fanny\_euclidean & \scriptsize    4    & \scriptsize basketball & \scriptsize    0.0157  &  \scriptsize 0.0032 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   4  & \scriptsize    3  & \scriptsize    3  & \scriptsize    3 \\
\scriptsize   fanny   & \scriptsize  fanny\_euclidean & \scriptsize    4    & \scriptsize basketball & \scriptsize    0.0157  &  \scriptsize 0.0032 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   2  & \scriptsize    5  & \scriptsize    4  & \scriptsize    4 \\
\scriptsize   fanny   & \scriptsize  fanny\_euclidean & \scriptsize    4    & \scriptsize basketball & \scriptsize    0.0178  &  \scriptsize 0.0000 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   5  & \scriptsize    1  & \scriptsize    5  & \scriptsize    5 \\
\cline{3-12}
\scriptsize   fanny   & \scriptsize  fanny\_euclidean & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0167  &  \scriptsize 0.2069 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   3  & \scriptsize    4  & \scriptsize    1  & \scriptsize    1 \\
\scriptsize   fanny   & \scriptsize  fanny\_euclidean & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0189  &  \scriptsize 0.1675 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   2  & \scriptsize    2  & \scriptsize    2  & \scriptsize    2 \\
\scriptsize   fanny   & \scriptsize  fanny\_euclidean & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0219  &  \scriptsize 0.0032 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   4  & \scriptsize    3  & \scriptsize    3  & \scriptsize    3 \\
\scriptsize   fanny   & \scriptsize  fanny\_euclidean & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0272  &  \scriptsize 0.0032 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   1  & \scriptsize    5  & \scriptsize    4  & \scriptsize    4 \\
\scriptsize   fanny   & \scriptsize  fanny\_euclidean & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0286  &  \scriptsize 0.0000 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   5  & \scriptsize    1  & \scriptsize    5  & \scriptsize    5 \\
\cline{2-12}
\scriptsize   fanny   & \scriptsize  fanny\_manhattan & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0165  &  \scriptsize 0.2143 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   3  & \scriptsize    4  & \scriptsize    1  & \scriptsize    1 \\
\scriptsize   fanny   & \scriptsize  fanny\_manhattan & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0184  &  \scriptsize 0.1658 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   1  & \scriptsize    2  & \scriptsize    2  & \scriptsize    2 \\
\scriptsize   fanny   & \scriptsize  fanny\_manhattan & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0196  &  \scriptsize 0.0032 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   4  & \scriptsize    3  & \scriptsize    3  & \scriptsize    3 \\
\scriptsize   fanny   & \scriptsize  fanny\_manhattan & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0247  &  \scriptsize 0.0032 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   5  & \scriptsize    5  & \scriptsize    4  & \scriptsize    4 \\
\scriptsize   fanny   & \scriptsize  fanny\_manhattan & \scriptsize    3    & \scriptsize basketball & \scriptsize    0.0719  &  \scriptsize 0.0000 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   2  & \scriptsize    1  & \scriptsize    5  & \scriptsize    5 \\
\cline{3-12}
\scriptsize   fanny   & \scriptsize  fanny\_manhattan & \scriptsize    4    & \scriptsize basketball & \scriptsize    0.0152  &  \scriptsize 0.2143 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   4  & \scriptsize    4  & \scriptsize    1  & \scriptsize    1 \\
\scriptsize   fanny   & \scriptsize  fanny\_manhattan & \scriptsize    4    & \scriptsize basketball & \scriptsize    0.0159  &  \scriptsize 0.1658 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   1  & \scriptsize    2  & \scriptsize    2  & \scriptsize    2 \\
\scriptsize   fanny   & \scriptsize  fanny\_manhattan & \scriptsize    4    & \scriptsize basketball & \scriptsize    0.0183  &  \scriptsize 0.0032 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   5  & \scriptsize    3  & \scriptsize    3  & \scriptsize    3 \\
\scriptsize   fanny   & \scriptsize  fanny\_manhattan & \scriptsize    4    & \scriptsize basketball & \scriptsize    0.0189  &  \scriptsize 0.0032 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   2  & \scriptsize    5  & \scriptsize    4  & \scriptsize    4 \\
\scriptsize   fanny   & \scriptsize  fanny\_manhattan & \scriptsize    4    & \scriptsize basketball & \scriptsize    0.0193  &  \scriptsize 0.0000 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   3  & \scriptsize    1  & \scriptsize    5  & \scriptsize    5 \\
\cline{3-12}
\scriptsize   fanny   & \scriptsize  fanny\_manhattan & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0178  &  \scriptsize 0.2143 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   4  & \scriptsize    4  & \scriptsize    1  & \scriptsize    1 \\
\scriptsize   fanny   & \scriptsize  fanny\_manhattan & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0183  &  \scriptsize 0.1658 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   1  & \scriptsize    2  & \scriptsize    2  & \scriptsize    2 \\
\scriptsize   fanny   & \scriptsize  fanny\_manhattan & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0207  &  \scriptsize 0.0032 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   3  & \scriptsize    3  & \scriptsize    3  & \scriptsize    3 \\
\scriptsize   fanny   & \scriptsize  fanny\_manhattan & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0211  &  \scriptsize 0.0032 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   2  & \scriptsize    5  & \scriptsize    4  & \scriptsize    4 \\
\scriptsize   fanny   & \scriptsize  fanny\_manhattan & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0211  &  \scriptsize 0.0000 & \scriptsize 0.0000 & \scriptsize    0.0000 & \scriptsize   5  & \scriptsize    1  & \scriptsize    5  & \scriptsize    5 \\
\hline
\caption{Results of running the main method of the package.}
\label{tab:clustering}
\end{longtable}

To make compression easier we have extracted the values of the
\texttt{gmm} algorithm with five clusters the and \texttt{manhattan}
measurement. The values are arranged in descending order. The values in
the columns ending in Attr refer to the attributes of the dataset. As
already mentioned, the attributes of the dataset are numbered from left
to right in ascending order. Thus \texttt{assists per minute} which is
the first attribute of the dataset found on the left is numbered with 1,
\texttt{height} with 2, \texttt{time played} with 3, \texttt{age} with 4
and \texttt{points per minute} with 5.

\begin{longtable}{| p{1.1cm} | p{2cm} | p{0.8cm} | p{1.3cm} | p{0.60cm} | p{0.7cm} | p{0.65cm} | p{0.65cm} | p{0.65cm} | p{0.65cm} | p{0.7cm} | p{0.65cm} |}
\hline
\scriptsize  Algorithm & \scriptsize  Distance  &  \scriptsize Clusters & \scriptsize  Dataset & \scriptsize tE & \scriptsize entropy & \scriptsize  dunn  & \scriptsize tI & \scriptsize tEAttr & \scriptsize enAttr & \scriptsize duAttr & \scriptsize tIAttr  \\
\hline
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0032  &  \scriptsize 0.4290 & \scriptsize 0.1141 & \scriptsize    0.0004 & \scriptsize   4  & \scriptsize    2  & \scriptsize    1  & \scriptsize    1 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0036  &  \scriptsize 0.3887 & \scriptsize 0.1141 & \scriptsize    0.0004 & \scriptsize   1  & \scriptsize    4  & \scriptsize    2  & \scriptsize    5 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0072  &  \scriptsize 0.0064 & \scriptsize 0.1141 & \scriptsize    0.0005 & \scriptsize   3  & \scriptsize    3  & \scriptsize    3  & \scriptsize    4 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0079  &  \scriptsize 0.0032 & \scriptsize 0.1141 & \scriptsize    0.0007 & \scriptsize   2  & \scriptsize    5  & \scriptsize    4  & \scriptsize    2 \\
\scriptsize     gmm   & \scriptsize    gmm\_manhattan & \scriptsize    5    & \scriptsize basketball & \scriptsize    0.0084  &  \scriptsize 0.0000 & \scriptsize 0.1141 & \scriptsize    0.0011 & \scriptsize   5  & \scriptsize    1  & \scriptsize    5  & \scriptsize    3 \\
\hline
\caption{Example of calculation of the attributes of a dataset by algorithm, means and cluster number.}
\label{tab:clusteringExample}
\end{longtable}

The name of the column \texttt{tE} corresponds to \texttt{timeExternal},
\texttt{tI} to \texttt{timeInternal}, \texttt{tEAttr} to
\texttt{timeExternalAttr}, \texttt{enAttr} to \texttt{entropyAttr},
\texttt{duAttr} to \texttt{dunnAttr} and finally \texttt{tIAttr} to
\texttt{timeInternalAttr}. From the data in Table
\textasciitilde{}\ref{tab:clusteringExample} in row one we have the
highest value of entropy for the basketball dataset. If we want to know
the dataset attribute for the maximum value we look in the equivalent
column of the metric, in our case as we are centered on \texttt{entropy}
we must look in the column \texttt{enAttr}, which indicates that it is
attribute two that corresponds to height.

In order to summarize the information this package provides a series of
mechanisms that allow us to summarize the information from the result
obtained by the clustering method in such a way that we can obtain the
best results and attributes for one or several data sets. The mechanisms
used are the following:

\begin{itemize}
\tightlist
\item
  \texttt{best\_ranked\_external\_metrics()}: Method used for selecting
  from the data set attributes those that obtain the best result in the
  evaluation of the measure. We will now use this mechanism on the
  previous results.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Clustering}\SpecialCharTok{::}\FunctionTok{best\_ranked\_external\_metrics}\NormalTok{(result)}
\end{Highlighting}
\end{Shaded}

\newpage
\begin{table}[h!]
\centering
\begin{tabular}{| p{1.3cm} | p{2.1cm} | p{0.9cm} | p{1.1cm} | p{1.6cm} | p{0.8cm} | p{2.1cm} | p{1.5cm} |}
\hline
\scriptsize Algorithm  & \scriptsize     Distance    &  \scriptsize Clusters &  \scriptsize  Dataset   & \scriptsize timeExternal  & \scriptsize entropy   & \scriptsize timeExternalAttr & \scriptsize entropyAttr \\
\hline
\scriptsize   gmm      & \scriptsize   gmm\_euclidean & \scriptsize     3     & \scriptsize basketball  & \scriptsize     0.0104    & \scriptsize 0.2374 &  \scriptsize        5        & \scriptsize      2 \\
\scriptsize   gmm      & \scriptsize   gmm\_euclidean & \scriptsize     4     & \scriptsize basketball  & \scriptsize     0.0147    & \scriptsize 0.3734 &  \scriptsize        2        & \scriptsize      2\\
\scriptsize   gmm      & \scriptsize   gmm\_euclidean & \scriptsize     5     & \scriptsize basketball  & \scriptsize     0.0058    & \scriptsize 0.4175 &  \scriptsize        5        & \scriptsize      2 \\
\cline{2-8}
\scriptsize   gmm      & \scriptsize   gmm\_manhattan & \scriptsize     3     & \scriptsize basketball  & \scriptsize     0.0066    & \scriptsize 0.2498 &  \scriptsize        5        & \scriptsize      2 \\
\scriptsize   gmm      & \scriptsize   gmm\_manhattan & \scriptsize     4     & \scriptsize basketball  & \scriptsize     0.0083    & \scriptsize 0.3563 &  \scriptsize        4        & \scriptsize      2 \\
\scriptsize   gmm      & \scriptsize   gmm\_manhattan & \scriptsize     5     & \scriptsize basketball  & \scriptsize     0.0045    & \scriptsize 0.4290 &  \scriptsize        5        & \scriptsize      2 \\
\hline
\scriptsize fanny      & \scriptsize fanny\_euclidean & \scriptsize     3     & \scriptsize basketball  & \scriptsize     0.0177    & \scriptsize 0.2069 &  \scriptsize        5        & \scriptsize      4 \\
\scriptsize fanny      & \scriptsize fanny\_euclidean & \scriptsize     4     & \scriptsize basketball  & \scriptsize     0.0148    & \scriptsize 0.2069 &  \scriptsize        5        & \scriptsize      4 \\
\scriptsize fanny      & \scriptsize fanny\_euclidean & \scriptsize     5     & \scriptsize basketball  & \scriptsize     0.0199    & \scriptsize 0.2069 &  \scriptsize        1        & \scriptsize      4 \\
\cline{2-8}
\scriptsize fanny      & \scriptsize fanny\_manhattan & \scriptsize     3     & \scriptsize basketball  & \scriptsize     0.0311    & \scriptsize 0.2143 &  \scriptsize        3        & \scriptsize      4 \\
\scriptsize fanny      & \scriptsize fanny\_manhattan & \scriptsize     4     & \scriptsize basketball  & \scriptsize     0.0210    & \scriptsize 0.2143 &  \scriptsize        4        & \scriptsize      4 \\
\scriptsize fanny      & \scriptsize fanny\_manhattan & \scriptsize     5     & \scriptsize basketball  & \scriptsize     0.0222    & \scriptsize 0.2143 &  \scriptsize        2        & \scriptsize      4 \\
\hline
\end{tabular}
\caption{Results of the execution of the \texttt{best\_ranked\_external\_metrics()}.}
\label{tab:clusteringbestrankedexternalmetrics}
\end{table}

In the calculation of the \texttt{entropy} the results are in the
interval {[}0,1{]}. For this example we have the best attributes in
\texttt{entropy} grouped by algorithm, measure of dissimilarity and
cluster number. When grouping the data, we choose the one with the
highest value. We perform the same calculation for internal
measurements.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Clustering}\SpecialCharTok{::}\FunctionTok{best\_ranked\_internal\_metrics}\NormalTok{(result)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}{| p{1.3cm} | p{2.1cm} | p{0.9cm} | p{1.1cm} | p{1.6cm} | p{0.8cm} | p{1.3cm} | p{1.9cm} |}
\hline
\scriptsize  Algorithm & \scriptsize     Distance     & \scriptsize Clusters & \scriptsize  Dataset  & \scriptsize timeInternal & \scriptsize  dunn  & \scriptsize dunnAttr & \scriptsize timeInternalAttr \\
\hline
\scriptsize     gmm    & \scriptsize   gmm\_euclidean & \scriptsize    3     & \scriptsize basketball & \scriptsize    0.0009    & \scriptsize 0.1096 & \scriptsize    1     & \scriptsize        5 \\
\scriptsize     gmm    & \scriptsize   gmm\_euclidean & \scriptsize    4     & \scriptsize basketball & \scriptsize    0.0008    & \scriptsize 0.1233 & \scriptsize    1     & \scriptsize        4 \\
\scriptsize     gmm    & \scriptsize   gmm\_euclidean & \scriptsize    5     & \scriptsize basketball & \scriptsize    0.0005    & \scriptsize 0.1619 & \scriptsize    1     & \scriptsize        2 \\
\cline{2-8}
\scriptsize     gmm    & \scriptsize   gmm\_manhattan & \scriptsize    3     & \scriptsize basketball & \scriptsize    0.0006    & \scriptsize 0.1151 & \scriptsize    1     & \scriptsize        1 \\
\scriptsize     gmm    & \scriptsize   gmm\_manhattan & \scriptsize    4     & \scriptsize basketball & \scriptsize    0.0005    & \scriptsize 0.1179 & \scriptsize    1     & \scriptsize        2 \\
\scriptsize     gmm    & \scriptsize   gmm\_manhattan & \scriptsize    5     & \scriptsize basketball & \scriptsize    0.0006    & \scriptsize 0.1141 & \scriptsize    1     & \scriptsize        5 \\
\hline
\scriptsize   fanny    & \scriptsize fanny\_euclidean & \scriptsize    3     & \scriptsize basketball & \scriptsize    0.0000    & \scriptsize 0.0000 & \scriptsize    1     & \scriptsize        1 \\
\scriptsize   fanny    & \scriptsize fanny\_euclidean & \scriptsize    4     & \scriptsize basketball & \scriptsize    0.0000    & \scriptsize 0.0000 & \scriptsize    1     & \scriptsize        1 \\
\scriptsize   fanny    & \scriptsize fanny\_euclidean & \scriptsize    5     & \scriptsize basketball & \scriptsize    0.0000    & \scriptsize 0.0000 & \scriptsize    1     & \scriptsize        1 \\
\cline{2-8}
\scriptsize   fanny    & \scriptsize fanny\_manhattan & \scriptsize    3     & \scriptsize basketball & \scriptsize    0.0000    & \scriptsize 0.0000 & \scriptsize    1     & \scriptsize        1 \\
\scriptsize   fanny    & \scriptsize fanny\_manhattan & \scriptsize    4     & \scriptsize basketball & \scriptsize    0.0000    & \scriptsize 0.0000 & \scriptsize    1     & \scriptsize        1 \\
\scriptsize   fanny    & \scriptsize fanny\_manhattan & \scriptsize    5     & \scriptsize basketball & \scriptsize    0.0000    & \scriptsize 0.0000 & \scriptsize    1     & \scriptsize        1 \\
\hline
\caption{Results of the execution of the \texttt{best\_ranked\_internal\_metrics()}.}
\label{tab:clusteringbestrankedinternalmetrics}
\end{longtable}

We already have the best attributes for each execution, and have
available methods for grouping the measures of dissimilarity from the
algorithms. When grouping the results by measures of dissimilarity and
algorithm we do not use a specific grouping algorithm, but rather we
keep those values whose value is the maximum depending on the type of
metric. In this case we see that for the \texttt{fanny} algorithm with
dissimilarity measure \texttt{Euclidean} and taking into account the
number of clusters, the value closest in \texttt{entropy} to 1 is
0.2090. For the rest of the algorithms the same process is followed.

\begin{itemize}
\tightlist
\item
  \texttt{evaluate\_best\_validation\_external\_by\_metrics()}: With
  this method we intend to demonstrate whether the choice of measurement
  of similarity has an influence. The results obtained in Table
  \textasciitilde{}\ref{tab:evaluatebestvalidationexternalbymetrics} are
  obtained by filtering the values of
  \texttt{best\_ranked\_external\_metrics()} and aggregating them by
  algorithm and measure of dissimilarity.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Clustering}\SpecialCharTok{::}\FunctionTok{evaluate\_best\_validation\_external\_by\_metrics}\NormalTok{(result)}
\end{Highlighting}
\end{Shaded}

\newpage
\begin{longtable}{| p{1.2cm} | p{2.2cm} |  p{1.8cm} | p{0.8cm} | p{2.1cm} | p{1.9cm} |}
\hline
\scriptsize  Algorithm & \scriptsize    Distance     & \scriptsize timeExternal & \scriptsize entropy & \scriptsize timeExternalAttr & \scriptsize entropyAttr \\
\hline
\scriptsize   fanny    & \scriptsize fanny\_euclidean & \scriptsize    0.0199    & \scriptsize  0.2069 & \scriptsize        5         & \scriptsize     4 \\
\cline{2-6}
\scriptsize   fanny    & \scriptsize fanny\_manhattan & \scriptsize    0.0311    & \scriptsize  0.2143 & \scriptsize        4         & \scriptsize     4 \\
\hline
\scriptsize    gmm     & \scriptsize  gmm\_euclidean  & \scriptsize    0.0147    & \scriptsize  0.4175 & \scriptsize        5         & \scriptsize     2 \\
\cline{2-6}
\scriptsize    gmm     & \scriptsize  gmm\_manhattan  & \scriptsize    0.0083    & \scriptsize   0.4290 & \scriptsize        5         & \scriptsize     2 \\
\hline
\caption{Results of execute \texttt{evaluate\_best\_validation\_external\_by\_metrics()}.}
\label{tab:evaluatebestvalidationexternalbymetrics}
\end{longtable}

If we observe Table
\textasciitilde{}\ref{tab:clusteringbestrankedexternalmetrics} we can
see that rows three and six give us the highest value for the
\texttt{gmm} algorithm and the measures \texttt{Euclidean} and
\texttt{Manhattan}. In the case of \texttt{fanny} all the attributes
return the same value, so we choose those whose execution time is lower.

\begin{itemize}
\tightlist
\item
  \texttt{evaluate\_validation\_external\_by\_metrics()}: If we want to
  go further and wish to determine the best algorithm from the
  attributes, we can do so in the following way.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Clustering}\SpecialCharTok{::}\FunctionTok{evaluate\_validation\_external\_by\_metrics}\NormalTok{(result)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}{| p{1.3cm} | p{1.6cm} | p{0.8cm} | p{2.2cm} | p{1.8cm} |}
\hline
\scriptsize Algorithm & \scriptsize timeExternal & \scriptsize entropy & \scriptsize timeExternalAttr & \scriptsize entropyAttr \\
\hline
\scriptsize   fanny   & \scriptsize    0.0311    & \scriptsize 0.2143  & \scriptsize        5         & \scriptsize     4 \\
\scriptsize    gmm    & \scriptsize    0.0147    & \scriptsize 0.4290  & \scriptsize        5         & \scriptsize     2  \\
\hline
\caption{Results of \texttt{evaluate\_validation\_external\_by\_metrics()} method.}
\label{tab:evaluatevalidationexternalbymetrics}
\end{longtable}

With \texttt{evaluate\_validation\_external\_by\_metrics} we can see
that the best suited algorithm for the dataset is \texttt{gmm}
algorithm, as the value of \texttt{entropy} is closer to 1.

\begin{itemize}
\tightlist
\item
  \texttt{result\_external\_algorithm\_by\_metric()}: This method has
  been incorporated to filter the results of the clustering object from
  an algorithm in order to be able to choose a suitable cluster.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Clustering}\SpecialCharTok{::}\FunctionTok{result\_external\_algorithm\_by\_metric}\NormalTok{(result,}\StringTok{\textquotesingle{}gmm\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}{| p{1.3cm} | p{1.5cm} | p{1.7cm} | p{1.2cm} | p{2.1cm} | p{1.8cm} |}
\hline
\scriptsize Algorithm & \scriptsize Clusters & \scriptsize timeExternal & \scriptsize entropy & \scriptsize timeExternalAttr & \scriptsize entropyAttr  \\
\hline
\scriptsize    gmm    & \scriptsize     3    & \scriptsize    0.0104    & \scriptsize 0.2498  & \scriptsize         5        & \scriptsize       2 \\
\scriptsize    gmm    & \scriptsize     4    & \scriptsize    0.0147    & \scriptsize 0.3734  & \scriptsize         4        & \scriptsize       2 \\
\scriptsize    gmm    & \scriptsize     5    & \scriptsize    0.0058    & \scriptsize  0.4290  & \scriptsize         5        & \scriptsize       2 \\
\hline
\caption{Results of the execution of the \texttt{result\_external\_algorithm\_by\_metric()}.}
\label{tab:resultexternalalgorithmbymetric}
\end{longtable}

\begin{itemize}
\tightlist
\item
  \texttt{plot\_clustering()}: Mechanism for representing the evaluation
  metrics based on the number of clusters, accelerating the process of
  choosing the best results, as shown below. In Figure we can see that,
  in the representation of dunn metric, no fanny algorithm appears. This
  is because the dunn values for fanny algorithm are zeros.
\end{itemize}

\hypertarget{fig:coolFig}{}
\includegraphics[width=0.5\textwidth,height=\textheight]{img/entropy.png}
\includegraphics[width=0.5\textwidth,height=\textheight]{img/dunn.png}

Graphic representation of internal and external evaluation measures
grouped by algorithm and number of clusters

In conclusion, and based on the results, we can say that the
\texttt{gmm} algorithm with five clusters and \texttt{Manhattan}
similarity measure is the one with the best behavior for the
\texttt{entropy} external metric. The attributes of the dataset that
return the best value are the two that correspond to the column
\texttt{heightInteger}. All these operations that we have carried out to
evaluate the external measures can be extrapolated to the internal ones
and obtain the necessary information for the appropriate choice of the
algorithm as well as the number of clusters.

\hypertarget{data-post-processing}{%
\subsection{Data post-processing}\label{data-post-processing}}

To conclude the definition of the methods of the package it is always
necessary to have a functionality that allows us to sort, filter the
information or export the results. To do this we will detail the methods
used:

\begin{itemize}
\tightlist
\item
  \texttt{sort(clustering\_object,column\_name):} Sometimes we need to
  sort the columns in order to find the maximum and minimum value. An
  example might be to sort the \texttt{entropy} column in ascending
  order to find the maximum and minimum value for the data set. The
  easiest way to perform the sorting by column is as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OtherTok{\textless{}{-}}\NormalTok{ Clustering}\SpecialCharTok{::}\FunctionTok{clustering}\NormalTok{(}\AttributeTok{df =}\NormalTok{ Clustering}\SpecialCharTok{::}\NormalTok{basketball, }\AttributeTok{min =} \DecValTok{3}\NormalTok{, }\AttributeTok{max=}\DecValTok{3}\NormalTok{, }
          \AttributeTok{algorithm =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}gmm\textquotesingle{}}\NormalTok{), }\AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}entropy\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}dunn\textquotesingle{}}\NormalTok{), }\AttributeTok{attributes =}\NormalTok{ T);}

\FunctionTok{sort}\NormalTok{(result,T,}\StringTok{\textquotesingle{}entropy\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}{| p{1.1cm} | p{2cm} | p{0.8cm} | p{1.3cm} | p{0.60cm} | p{0.7cm} | p{0.65cm} | p{0.65cm} | p{0.65cm} | p{0.65cm} | p{0.7cm} | p{0.65cm} |}
\hline
\scriptsize  Algorithm & \scriptsize  Distance  &  \scriptsize Clusters & \scriptsize  Dataset & \scriptsize tE & \scriptsize entropy & \scriptsize  dunn  & \scriptsize tI & \scriptsize tEAttr & \scriptsize enAttr & \scriptsize duAttr & \scriptsize tIAttr  \\
\hline
\scriptsize gmm & \scriptsize gmm\_manhattan & \scriptsize 3 & \scriptsize basketball & \scriptsize 0.0026 & \scriptsize 0.2498 & \scriptsize 0.1151 & \scriptsize 0.0004 & \scriptsize 5 & \scriptsize 2 & \scriptsize 1 & \scriptsize 1 \\
\cline{2-12}
\scriptsize gmm & \scriptsize gmm\_euclidean & \scriptsize 3 & \scriptsize basketball & \scriptsize 0.0040 & \scriptsize 0.2374 & \scriptsize 0.1096 & \scriptsize 0.0004 & \scriptsize 5 & \scriptsize 2 & \scriptsize 1 & \scriptsize 1 \\
\cline{2-12}
\scriptsize gmm & \scriptsize gmm\_manhattan & \scriptsize 3 & \scriptsize basketball & \scriptsize 0.0033 & \scriptsize 0.2201 & \scriptsize 0.1151 & \scriptsize 0.0004 & \scriptsize 2 & \scriptsize 4 & \scriptsize 2 & \scriptsize 2 \\
\cline{2-12}
\scriptsize gmm & \scriptsize gmm\_euclidean & \scriptsize 3 & \scriptsize basketball & \scriptsize 0.0041 & \scriptsize 0.2120 & \scriptsize 0.1096 & \scriptsize 0.0004 & \scriptsize 1 & \scriptsize 4 & \scriptsize 2 & \scriptsize 2 \\
\cline{2-12}
\scriptsize gmm & \scriptsize gmm\_euclidean & \scriptsize 3 & \scriptsize basketball & \scriptsize 0.0074 & \scriptsize 0.0064 & \scriptsize 0.1096 & \scriptsize 0.0004 & \scriptsize 3 & \scriptsize 3 & \scriptsize 3 & \scriptsize 5 \\
\cline{2-12}
\scriptsize gmm & \scriptsize gmm\_manhattan & \scriptsize 3 & \scriptsize basketball & \scriptsize 0.0059 & \scriptsize 0.0064 & \scriptsize 0.1151 & \scriptsize 0.0004 & \scriptsize 3 & \scriptsize 3 & \scriptsize 3 & \scriptsize 3 \\
\cline{2-12}
\scriptsize gmm & \scriptsize gmm\_euclidean & \scriptsize 3 & \scriptsize basketball & \scriptsize 0.0074 & \scriptsize 0.0032 & \scriptsize 0.1096 & \scriptsize 0.0004 & \scriptsize 2 & \scriptsize 5 & \scriptsize 4 & \scriptsize 3 \\
\cline{2-12}
\scriptsize gmm & \scriptsize gmm\_manhattan & \scriptsize 3 & \scriptsize basketball & \scriptsize 0.0061 & \scriptsize 0.0032 & \scriptsize 0.1151 & \scriptsize 0.0004 & \scriptsize 1 & \scriptsize 5 & \scriptsize 4 & \scriptsize 4 \\
\cline{2-12}
\scriptsize gmm & \scriptsize gmm\_euclidean & \scriptsize 3 & \scriptsize basketball & \scriptsize 0.0083 & \scriptsize 0.0000 & \scriptsize 0.1096 & \scriptsize 0.0006 & \scriptsize 4 & \scriptsize 1 & \scriptsize 5 & \scriptsize 4 \\
\cline{2-12}
\scriptsize gmm & \scriptsize gmm\_manhattan & \scriptsize 3 & \scriptsize basketball & \scriptsize 0.0065 & \scriptsize 0.0000 & \scriptsize 0.1151 & \scriptsize 0.0004 & \scriptsize 4 & \scriptsize 1 & \scriptsize 5 & \scriptsize 5 \\
\hline
\caption{Sort results by \texttt{entropy} column.}
\label{tab:resultsorting}
\end{longtable}

\begin{itemize}
\tightlist
\item
  \texttt{"[.clustering":} There are times when we need to apply filters
  on a series of columns for a set of values. This process can be
  carried out using third party packages (\texttt{dplyr}), but due to
  its great usefulness we have incorporated this functionality. We must
  filter it in the following way:
  \texttt{clustering\_object [column\_1 operator value\_1 conditional\_1 .... column\_n operator value\_n]}.
  Example of filtering:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result[entropy }\SpecialCharTok{\textgreater{}} \FloatTok{0.11} \SpecialCharTok{\&}\NormalTok{ dunn }\SpecialCharTok{\textgreater{}} \FloatTok{0.11} \SpecialCharTok{\&}\NormalTok{ entropyAttr }\SpecialCharTok{==} \DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}{| p{1.1cm} | p{2cm} | p{0.8cm} | p{1.3cm} | p{0.60cm} | p{0.7cm} | p{0.65cm} | p{0.65cm} | p{0.65cm} | p{0.65cm} | p{0.7cm} | p{0.65cm} |}
\hline
\scriptsize  Algorithm & \scriptsize  Distance  &  \scriptsize Clusters & \scriptsize  Dataset & \scriptsize tE & \scriptsize entropy & \scriptsize  dunn  & \scriptsize tI & \scriptsize tEAttr & \scriptsize enAttr & \scriptsize duAttr & \scriptsize tIAttr  \\
\hline
\scriptsize    gmm     & \scriptsize  gmm\_manhattan & \scriptsize      3  & \scriptsize   basketball  & \scriptsize   0.0067  & \scriptsize  0.2498  & \scriptsize 0.1151  & \scriptsize    0.0008     & \scriptsize     5       & \scriptsize       2    & \scriptsize      1      & \scriptsize      3 \\
\hline
\caption{Filtering information by different criteria.}
\label{tab:filtering}
\end{longtable}

\begin{itemize}
\tightlist
\item
  \texttt{export\_file\_external():} Exports the results of the
  clustering object to \LaTeX format. This method is very useful when
  working with documents in \LaTeX format.
\item
  \texttt{export\_file\_internal():} This method is similar to the
  previous one, but only exports the internal metrics.
\end{itemize}

\hypertarget{graphical-user-interface-of-the-clustering-package}{%
\section{\texorpdfstring{Graphical User Interface of the
\textbf{Clustering}
package}{Graphical User Interface of the Clustering package}}\label{graphical-user-interface-of-the-clustering-package}}

As mentioned throughout this paper, the \textbf{Clustering} package
provides a GUI in order to work with clustering algorithms and to be
able to evaluate and run the results more comfortably. The user
interface is run by executing the following instruction:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Clustering}\SpecialCharTok{::}\FunctionTok{appClustering}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The execution will open our default browser with the interface. As it
can be observed in Figure \textasciitilde{}\ref{layoutApp}, we have a
layout with header, side menu and main menu. In the header menu we can
choose to see the results numerically or in graphical mode. In the left
menu we can see the different parameters with which we can run our
algorithm, and finally in the central menu we can see the result of
running the clustering algorithm.

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=2.8125in]{img/app}
\caption{Clustering app user interface. \label{layoutApp}}
\end{figure}

The operation of the application is very simple, as can be seen in
Figure \textasciitilde{}\ref{app1} and we will proceed to explain it
step by step. As can be seen in Figure \textasciitilde{}\ref{app1} we
have two well differentiated parts:

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=3.4375in]{img/app1}
\caption{Clustering app user interface. \label{app1}}
\end{figure}

\begin{itemize}
\tightlist
\item
  In this section we can find the different parameters used by the
  clustering function to filter the information.

  \begin{itemize}
  \tightlist
  \item
    Marked in red, we can indicate if we want to work with test datasets
    or indicate a directory of dataset files to be processed.
  \item
    In blue we have the packages that implement the clustering
    algorithms mentioned throughout the paper. We can mark all packages
    or a subset of them. When a package is marked, all the algorithms
    implemented within the selected package are marked.
  \item
    In yellow we have the algorithms implemented by the packages. If we
    mark an algorithm it will automatically mark its corresponding
    package in the package combo.
  \item
    In green we have the number of clusters. We can indicate ranges or
    select only one cluster by positioning the maximum and minimum on
    the same value.
  \item
    In violet we indicate the evaluation metrics used when validating
    the clusters.
  \item
    Finally we show the attributes of the dataset that correspond to the
    values indicated in each of the metrics. \clearpage
  \end{itemize}
\item
  In the main layout we have the options for representing the data.

  \begin{itemize}
  \tightlist
  \item
    If we click on the summary tab as shown in Figure
    \textasciitilde{}\ref{tab_summary}, we can see the data represented
    in tables. If we wish we can export the results in the following
    formats: CSV, PDF and XLS. We also have the option of copying the
    data.
  \end{itemize}
\end{itemize}

\begin{figure}
\centering
\includegraphics{img/tab_summary}
\caption{\textbf{Clustering} package execution summary tab..
\label{tab_summary}}
\end{figure}

\begin{itemize}
\tightlist
\item
  To view the data in graphical mode as shown in Figure
  \textasciitilde{}\ref{tab_graph}, we mark the Plot tab. In the Figure
  we can see represented the internal and external evaluation metrics
  and depending on the type of evaluation we can filter individually by
  metrics to see the data represented graphically.
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=3.54167in]{img/tab_graph}
\caption{Tab with graphical representation of metrics.
\label{tab_graph}}
\end{figure}

\newpage

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

In this paper we have introduced the \textbf{Clustering} package. The
package has dependencies on other packages, as seen throughout the
paper. It allows to read and load of datasets in KEEL, CSV or ARFF
format. We offer the functionality of loading a \texttt{data.frame} in
memory or using test datasets. As a complement, the package has been
enhanced with the inclusion of a graphical interface that allows the
user to run the package in a simple way without the need to know the
parameters. With the package we contribute to choosing which variable of
a dataset is the one that obtains the best value when we evaluate the
clusters, as well as applying relative criteria that allow us to compare
the execution results of an algorithm with another one using different
parameters. The development of the package will be continued with the
inclusion of new algorithms, functionalities and improvement of the
interface, and therefore we encourage developers to contribute to the
improvement of the package with the inclusion of new algorithms or
functionalities or the inclusion of new proposals to complement the
package.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\leavevmode\hypertarget{ref-Rmannkaur2013}{}%
\CSLLeftMargin{{[}1{]} }
\CSLRightInline{A. Mann and N. Kaur, {``Paper on clustering techniques
by amandeep kaur mann \& navneet,''} 2013.}

\leavevmode\hypertarget{ref-Rmichaelgeorgevipin2000}{}%
\CSLLeftMargin{{[}2{]} }
\CSLRightInline{M. Steinbach, G. Karypis, and V. Kumar, {``A comparison
of document clustering techniques,''} \emph{Proceedings of the
International KDD Workshop on Text Mining}, 2000.}

\leavevmode\hypertarget{ref-Rhasnatmehedisamiul2018}{}%
\CSLLeftMargin{{[}3{]} }
\CSLRightInline{M. Hasnat and S. Hasan, {``Identifying tourists and
analyzing spatial patterns of their destinations from location-based
social media data,''} \emph{Transportation Research Part C Emerging
Technologies}, vol. 96, pp. 38--54, 2018, doi:
\href{https://doi.org/10.1016/j.trc.2018.09.006}{10.1016/j.trc.2018.09.006}.}

\leavevmode\hypertarget{ref-Rbalevigitlin2018}{}%
\CSLLeftMargin{{[}4{]} }
\CSLRightInline{E. Balevi and R. Gitlin, {``A clustering algorithm that
maximizes throughput in 5G heterogeneous f-RAN networks,''} \emph{2018
IEEE International Conference on Communications (ICC)}, pp. 1--6, 2018,
doi:
\href{https://doi.org/10.1109/ICC.2018.8422151}{10.1109/ICC.2018.8422151}.}

\leavevmode\hypertarget{ref-Rpatelhetaldharmendra2014}{}%
\CSLLeftMargin{{[}5{]} }
\CSLRightInline{H. Patel and D. Patel, {``A brief survey of data mining
techniques applied to agricultural data,''} \emph{International Journal
of Computer Applications}, vol. 95, no. 9, 2014, doi:
\href{https://doi.org/10.5120/16620-6472}{10.5120/16620-6472}.}

\leavevmode\hypertarget{ref-Rwangyiliyoun2019}{}%
\CSLLeftMargin{{[}6{]} }
\CSLRightInline{Y. Wang and H. Y. Youn, {``Feature weighting based on
inter-category and intra-category strength for twitter sentiment
analysis,''} \emph{Applied Sciences}, vol. 9, no. 1, p. 92, 2019, doi:
\href{https://doi.org/10.3390/app9010092}{10.3390/app9010092}.}

\leavevmode\hypertarget{ref-Rvlegelslievens2017}{}%
\CSLLeftMargin{{[}7{]} }
\CSLRightInline{J. Vlegels and J. Lievens, {``Music classification,
genres, and taste patterns: A ground-up network analysis on the
clustering of artist preferences,''} \emph{Poetics}, vol. 60, no. 1, pp.
76--89, 2017, doi:
\href{https://doi.org/10.1016/j.poetic.2016.08.004}{10.1016/j.poetic.2016.08.004}.}

\leavevmode\hypertarget{ref-Rdennysmallquifernandes2019}{}%
\CSLLeftMargin{{[}8{]} }
\CSLRightInline{D. C. A. Mallqui and R. Fernandes, {``Predicting the
direction, maximum, minimum and closing prices of daily bitcoin exchange
rate using machine learning techniques,''} \emph{Appl. Soft Comput.},
vol. 75, pp. 596--606, 2019, doi:
\href{https://doi.org/10.1016/j.asoc.2018.11.038}{10.1016/j.asoc.2018.11.038}.}

\leavevmode\hypertarget{ref-Rsinghshakyabiswas2016}{}%
\CSLLeftMargin{{[}9{]} }
\CSLRightInline{K. Singh, H. K. Shakya, and B. Biswas, {``Clustering of
people in social network based on textual similarity,''}
\emph{Perspectives in Science}, vol. 8, pp. 570--573, 2016, doi:
\href{https://doi.org/10.1016/j.pisc.2016.06.023}{10.1016/j.pisc.2016.06.023}.}

\leavevmode\hypertarget{ref-Rshraddhakm2014}{}%
\CSLLeftMargin{{[}10{]} }
\CSLRightInline{S. K. Popat and M. Emmanuel, {``Review and comparative
study of clustering techniques,''} \emph{International journal of
computer science and information technologies}, vol. 5, no. 1, pp.
805--812, 2014.}

\leavevmode\hypertarget{ref-Rmacqueen1967}{}%
\CSLLeftMargin{{[}11{]} }
\CSLRightInline{J. Macqueen, {``Some methods for classification and
analysis of multivariate observations,''} in \emph{In 5-th berkeley
symposium on mathematical statistics and probability}, 1967, pp.
281--297.}

\leavevmode\hypertarget{ref-Rnarendraamanratnesh2012}{}%
\CSLLeftMargin{{[}12{]} }
\CSLRightInline{N. Sharma, A. Bajpai, and R. Litoriya, {``Comparison the
various clustering algorithms of weka tools,''} 2012.}

\leavevmode\hypertarget{ref-Rtaunojaak2015}{}%
\CSLLeftMargin{{[}13{]} }
\CSLRightInline{T. Metsalu and J. Vilo, {``ClustVis: A web tool for
visualizing clustering of multivariate data using principal component
analysis and heatmap,''} \emph{Nucleic acids research}, vol. 43, 2015,
doi: \href{https://doi.org/10.1093/nar/gkv468}{10.1093/nar/gkv468}.}

\leavevmode\hypertarget{ref-Ralbertojulianjoaquinjesusfrancisco2009}{}%
\CSLLeftMargin{{[}14{]} }
\CSLRightInline{A. Fernández, J. Luengo, J. Derrac, J. Alcala-Fdez, and
F. Herrera, {``Implementation and integration of algorithms into the
KEEL data-mining software tool,''} 2009, vol. 5788, pp. 562--569, doi:
\href{https://doi.org/10.1007/978-3-642-04394-9_68}{10.1007/978-3-642-04394-9\_68}.}

\leavevmode\hypertarget{ref-Rsculley2010}{}%
\CSLLeftMargin{{[}15{]} }
\CSLRightInline{D. Sculley, {``Web-scale k-means clustering,''} 2010,
pp. 1177--1178, doi:
\href{https://doi.org/10.1145/1772690.1772862}{10.1145/1772690.1772862}.}

\leavevmode\hypertarget{ref-Rmouselimis2020}{}%
\CSLLeftMargin{{[}16{]} }
\CSLRightInline{L. Mouselimis, \emph{ClusterR: Gaussian mixture models,
k-means, mini-batch-kmeans, k-medoids and affinity propagation
clustering}. 2020.}

\leavevmode\hypertarget{ref-Rfreydueck2007}{}%
\CSLLeftMargin{{[}17{]} }
\CSLRightInline{B. J. Frey and D. Dueck, {``Clustering by passing
messages between data points,''} \emph{Science}, vol. 315, pp. 972--977,
2007, doi:
\href{https://doi.org/10.1126/science.1136800}{10.1126/science.1136800}.}

\leavevmode\hypertarget{ref-Rbodenhoferkothmeierhochreiter2011}{}%
\CSLLeftMargin{{[}18{]} }
\CSLRightInline{U. Bodenhofer, A. Kothmeier, and S. Hochreiter,
{``APCluster: An r package for affinity propagation clustering,''}
\emph{Bioinformatics}, vol. 27, pp. 2463--2464, 2011, doi:
\href{https://doi.org/10.1093/bioinformatics/btr406}{10.1093/bioinformatics/btr406}.}

\leavevmode\hypertarget{ref-Rmaechlerrousseeuwhuberthornik2019}{}%
\CSLLeftMargin{{[}19{]} }
\CSLRightInline{M. Maechler, P. Rousseeuw, A. Struyf, M. Hubert, and K.
Hornik, \emph{Cluster: Cluster analysis basics and extensions}. 2019.}

\leavevmode\hypertarget{ref-Rfariasubiernaelorzasantamarialaso2011}{}%
\CSLLeftMargin{{[}20{]} }
\CSLRightInline{C. Correa, C. Valero, P. Barreiro, M.-P. Diago, and J.
Tardaguila, {``A comparison of fuzzy clustering algorithms applied to
feature extraction on vineyard,''} in \emph{Inteligencia artificial
revista iberoamericana de inteligencia artificial}, 2011, vol. 1, p.
778.}

\leavevmode\hypertarget{ref-Rsotirisp2004}{}%
\CSLLeftMargin{{[}21{]} }
\CSLRightInline{S. Kotsiantis and P. Pintelas, {``Recent advances in
clustering: A brief survey,''} \emph{WSEAS Transactions on Information
Science and Applications}, vol. 1, pp. 73--81, 2004.}

\leavevmode\hypertarget{ref-Rmahamedandriesayed2007}{}%
\CSLLeftMargin{{[}22{]} }
\CSLRightInline{M. Omran, A. Engelbrecht, and A. Salman, {``An overview
of clustering methods,''} \emph{Intell. Data Anal.}, vol. 11, pp.
583--605, 2007, doi:
\href{https://doi.org/10.3233/IDA-2007-11602}{10.3233/IDA-2007-11602}.}

\leavevmode\hypertarget{ref-Rpatibandlalakshmiveeranjaneyulu2018}{}%
\CSLLeftMargin{{[}23{]} }
\CSLRightInline{R. S. M. L. Patibandla and N. Veeranjaneyulu, {``Survey
on clustering algorithms for unstructured data,''} 2018, pp. 421--429,
doi:
\href{https://doi.org/10.1007/978-981-10-7566-7_41}{10.1007/978-981-10-7566-7\_41}.}

\leavevmode\hypertarget{ref-Rjainmurtyflynn1999}{}%
\CSLLeftMargin{{[}24{]} }
\CSLRightInline{A. K. Jain, M. Murty, and P. Flynn, {``Data clustering:
A review,''} \emph{ACM Comput. Surv.}, vol. 31, pp. 264--323, 1999.}

\leavevmode\hypertarget{ref-Rguharastogishim2001}{}%
\CSLLeftMargin{{[}25{]} }
\CSLRightInline{S. Guha, R. Rastogi, and K. Shim, {``Cure: An efficient
clustering algorithm for large databases,''} \emph{Information Systems},
vol. 26, no. 1, pp. 35--58, 2001, {[}Online{]}. Available:
\url{http://www.sciencedirect.com/science/article/pii/S0306437901000084}.}

\leavevmode\hypertarget{ref-Rdongweijingjingjici2019}{}%
\CSLLeftMargin{{[}26{]} }
\CSLRightInline{D. Guo, J. Zhao, and J. Liu, {``Research and application
of improved CHAMELEON algorithm based on condensed hierarchical
clustering method,''} 2019, pp. 14--18, doi:
\href{https://doi.org/10.1145/3375998.3376016}{10.1145/3375998.3376016}.}

\leavevmode\hypertarget{ref-Rtianraghumiron1996}{}%
\CSLLeftMargin{{[}27{]} }
\CSLRightInline{T. Zhang, R. Ramakrishnan, and M. Livny, {``BIRCH: An
efficient data clustering method for very large databases,''} 1996, pp.
103--114, doi:
\href{https://doi.org/10.1145/233269.233324}{10.1145/233269.233324}.}

\leavevmode\hypertarget{ref-Rha2019}{}%
\CSLLeftMargin{{[}28{]} }
\CSLRightInline{H. Ramprasanth and A. Devi, {``Outlier analysis of
medical dataset using clustering algorithms,''} \emph{Journal of
Analysis and Computation ISSN:(0973-2861)}, pp. 1--9, 2019.}

\leavevmode\hypertarget{ref-Rnithyaprabha2019}{}%
\CSLLeftMargin{{[}29{]} }
\CSLRightInline{G. Nithya and K. A. Prabha, {``A LION OPTIMIZATION BASED
k-PROTOTYPE CLUSTERING ALGORITHM FOR MIXED DATA,''} 2019.}

\leavevmode\hypertarget{ref-Rhuang1997}{}%
\CSLLeftMargin{{[}30{]} }
\CSLRightInline{J. Huang, {``A fast clustering algorithm to cluster very
large categorical data sets in data mining,''} 1997.}

\leavevmode\hypertarget{ref-Rmohithimanshuchetan2020}{}%
\CSLLeftMargin{{[}31{]} }
\CSLRightInline{M. Kushwaha, H. Yadav, and C. Agrawal, {``A review on
enhancement to standard k-means clustering,''} 2020, pp. 313--326.}

\leavevmode\hypertarget{ref-Rshan2019}{}%
\CSLLeftMargin{{[}32{]} }
\CSLRightInline{S. Hu, {``Indoor location method based on data
mining,''} 2019, pp. 11--15, doi:
\href{https://doi.org/10.1145/3377458.3377465}{10.1145/3377458.3377465}.}

\leavevmode\hypertarget{ref-Rmariamghazi2019}{}%
\CSLLeftMargin{{[}33{]} }
\CSLRightInline{M. Khader and G. Al-Naymat, {``An overview of various
enhancements of DENCLUE algorithm,''} 2019, pp. 1--7.}

\leavevmode\hypertarget{ref-Rshilpa2011}{}%
\CSLLeftMargin{{[}34{]} }
\CSLRightInline{S. Dang, {``A REVIEW OF CLUSTERING TECHIQUES IN VARIOUS
APPLICATIONS FOR EFFECTIVE DATA MINING,''} \emph{International Journal
of Research in IT \& Management 2231-4434}, vol. 1, pp. 50--66, 2011.}

\leavevmode\hypertarget{ref-Rilangodr2010}{}%
\CSLLeftMargin{{[}35{]} }
\CSLRightInline{I. MR and D. MOHAN, {``A survey of grid based clustering
algorithms,''} \emph{International Journal of Engineering Science and
Technology}, vol. 2, 2010.}

\leavevmode\hypertarget{ref-Rliliu2010}{}%
\CSLLeftMargin{{[}36{]} }
\CSLRightInline{L. Y. L. Xuecheng, {``APPLYING WAVE CLUSTER ALGORITHM IN
INTRUSION DETECTION {[}j{]},''} \emph{Computer Applications and
Software}, vol. 6, 2010.}

\leavevmode\hypertarget{ref-Rsainirani2017}{}%
\CSLLeftMargin{{[}37{]} }
\CSLRightInline{S. Saini and P. Rani, {``A survey on STING and CLIQUE
grid based clustering methods,''} \emph{International Journal of
Advanced Research in Computer Science}, vol. 8, pp. 1510--1512, 2017.}

\leavevmode\hypertarget{ref-Ranbupalamindranilxiaojinggeorge2006}{}%
\CSLLeftMargin{{[}38{]} }
\CSLRightInline{A. Thalamuthu, I. Mukhopadhyay, X. Zheng, and G. Tseng,
{``Evaluation and comparison of gene clustering methods in microarray
analysis,''} \emph{Bioinformatics (Oxford, England)}, vol. 22, pp.
2405--12, 2006, doi:
\href{https://doi.org/10.1093/bioinformatics/btl406}{10.1093/bioinformatics/btl406}.}

\leavevmode\hypertarget{ref-Rshraddhasuchitaothers2011}{}%
\CSLLeftMargin{{[}39{]} }
\CSLRightInline{S. Pandit, S. Gupta, and others, {``A comparative study
on distance measuring approaches for clustering,''} \emph{International
Journal of Research in Computer Science}, vol. 2, no. 1, pp. 29--31,
2011.}

\leavevmode\hypertarget{ref-Raliseyedsaeedteh2015}{}%
\CSLLeftMargin{{[}40{]} }
\CSLRightInline{A. S. Shirkhorshidi, S. Aghabozorgi, and T. Ying Wah,
{``{A Comparison study on similarity and dissimilarity measures in
clustering continuous data},''} \emph{PLoS ONE}, vol. 10, no. 12, pp.
1--20, 2015.}

\leavevmode\hypertarget{ref-Rganmawu2007}{}%
\CSLLeftMargin{{[}41{]} }
\CSLRightInline{G. Gan, C. Ma, and J. Wu, {``Data clustering {} theory,
algorithms, and applications,''} 2007.}

\leavevmode\hypertarget{ref-Rruidonald2005}{}%
\CSLLeftMargin{{[}42{]} }
\CSLRightInline{R. Xu and D. Wunsch, {``Survey of clustering
algorithms,''} \emph{Neural Networks, IEEE Transactions on}, vol. 16,
pp. 645--678, 2005, doi:
\href{https://doi.org/10.1109/TNN.2005.845141}{10.1109/TNN.2005.845141}.}

\leavevmode\hypertarget{ref-Rinstitute2015}{}%
\CSLLeftMargin{{[}43{]} }
\CSLRightInline{T. O. Institute, {``{Learn About Pearson{\&}{\#}8217;s
Correlation Coefficient in SPSS With Data From the Global Health
Observatory Data (2012)},''} no. 2012, 2015, doi:
\url{https://dx.doi.org/10.4135/9781473948167}.}

\leavevmode\hypertarget{ref-Rsven2016}{}%
\CSLLeftMargin{{[}44{]} }
\CSLRightInline{S. Kosub, {``A note on the triangle inequality for the
jaccard distance,''} \emph{arXiv.org}, vol. 120, 2016, doi:
\href{https://doi.org/10.1016/j.patrec.2018.12.007}{10.1016/j.patrec.2018.12.007}.}

\leavevmode\hypertarget{ref-Rjasminenittinmadhura2016}{}%
\CSLLeftMargin{{[}45{]} }
\CSLRightInline{J. Irani, N. Pise, and M. Phatak, {``Clustering
techniques and the similarity measures used in clustering{} a survey,''}
\emph{International Journal of Computer Applications}, vol. 134, pp.
9--14, 2016, doi:
\href{https://doi.org/10.5120/ijca2016907841}{10.5120/ijca2016907841}.}

\leavevmode\hypertarget{ref-Rolegas2013}{}%
\CSLLeftMargin{{[}46{]} }
\CSLRightInline{O. Niakšu, {``Calculating distance measure for
clustering in multi-relational settings,''} 2013.}

\leavevmode\hypertarget{ref-Rmariayannismichalis2001}{}%
\CSLLeftMargin{{[}47{]} }
\CSLRightInline{M. Halkidi, Y. Batistakis, and M. Vazirgiannis, {``On
clustering validation techniques,''} \emph{Journal of intelligent
information systems}, vol. 17, no. 2--3, pp. 107--145, 2001.}

\leavevmode\hypertarget{ref-Rgordonsja2011}{}%
\CSLLeftMargin{{[}48{]} }
\CSLRightInline{G. S. Linoff and M. J. Berry, \emph{Data mining
techniques: For marketing, sales, and customer relationship management}.
John Wiley \& Sons, 2011.}

\leavevmode\hypertarget{ref-Rarnoldeanwothers2002}{}%
\CSLLeftMargin{{[}49{]} }
\CSLRightInline{R. A. Johnson, D. W. Wichern, and others, \emph{Applied
multivariate statistical analysis}, vol. 5. Prentice hall Upper Saddle
River, NJ, 2002.}

\leavevmode\hypertarget{ref-Rhyunsoohaesun2007}{}%
\CSLLeftMargin{{[}50{]} }
\CSLRightInline{H. Kim and H. Park, {``{Sparse non-negative matrix
factorizations via alternating non-negativity-constrained least squares
for microarray data analysis},''} \emph{Bioinformatics}, vol. 23, pp.
1495--1502, 2007, doi:
\href{https://doi.org/10.1093/bioinformatics/btm134}{10.1093/bioinformatics/btm134}.}

\leavevmode\hypertarget{ref-Rjanuszyousef2019}{}%
\CSLLeftMargin{{[}51{]} }
\CSLRightInline{J. Kacprzyk and Y. Farhaoui, \emph{Big data and smart
digital environment}. 2019.}

\leavevmode\hypertarget{ref-Rsimonenguyenjameskarin2016}{}%
\CSLLeftMargin{{[}52{]} }
\CSLRightInline{S. Romano, N. the vinh, J. Bailey, and K. Verspoor,
{``Adjusting for chance clustering comparison measures,''} \emph{Journal
of Machine Learning Research}, vol. 17, pp. 1--32, 2016.}

\leavevmode\hypertarget{ref-Rzengyouxiaofeishengchun2005}{}%
\CSLLeftMargin{{[}53{]} }
\CSLRightInline{Z. He, X. Xu, and S. Deng, {``K-ANMI{} a mutual
information based clustering algorithm for categorical data,''}
\emph{Information Fusion}, vol. 9, pp. 223--233, 2005, doi:
\href{https://doi.org/10.1016/j.inffus.2006.05.006}{10.1016/j.inffus.2006.05.006}.}

\leavevmode\hypertarget{ref-Rdeborahbaskarankannan2010}{}%
\CSLLeftMargin{{[}54{]} }
\CSLRightInline{L. J. Deborah, R. Baskaran, and A. Kannan, {``A survey
on internal validity measure for cluster validation,''}
\emph{International Journal of Computer Science and Engineering Survey},
vol. 1, pp. 85--102, 2010, doi:
\href{https://doi.org/10.5121/ijcses.2010.1207}{10.5121/ijcses.2010.1207}.}

\leavevmode\hypertarget{ref-Rzahidmfwaseema2015}{}%
\CSLLeftMargin{{[}55{]} }
\CSLRightInline{Z. Ansari, M. F. Azeem, W. Ahmed, and A. Babu,
{``Quantitative evaluation of performance and validity indices for
clustering the web navigational sessions,''} \emph{World of Computer
Science and Information Technology Journal}, vol. 1, 2015.}

\leavevmode\hypertarget{ref-Rarturadam2015}{}%
\CSLLeftMargin{{[}56{]} }
\CSLRightInline{A. Starczewski and A. Krzyżak, {``Performance evaluation
of the silhouette index,''} \emph{Lecture Notes in Artificial
Intelligence (Subseries of Lecture Notes in Computer Science)}, vol.
9120, pp. 49--58, 2015, doi:
\href{https://doi.org/10.1007/978-3-319-19369-4-5}{10.1007/978-3-319-19369-4-5}.}

\leavevmode\hypertarget{ref-Rpalaciogaliano2019}{}%
\CSLLeftMargin{{[}57{]} }
\CSLRightInline{J.-O. Palacio-Niño and F. Galiano, {``Evaluation metrics
for unsupervised learning algorithms,''} \emph{ArXiv}, vol.
abs/1905.05667, 2019.}

\leavevmode\hypertarget{ref-Rchartedavid2015}{}%
\CSLLeftMargin{{[}58{]} }
\CSLRightInline{F. Charte and D. Charte, {``Working with multilabel
datasets in {R}: The mldr package,''} \emph{The R Journal}, vol. 7, no.
2, pp. 149--162, 2015.}

\end{CSLReferences}

\end{document}
